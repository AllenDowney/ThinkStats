{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56aaf47a",
   "metadata": {},
   "source": [
    "# Probability density functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c252c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79f64f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import basename, exists\n",
    "\n",
    "\n",
    "def download(url):\n",
    "    filename = basename(url)\n",
    "    if not exists(filename):\n",
    "        from urllib.request import urlretrieve\n",
    "\n",
    "        local, _ = urlretrieve(url, filename)\n",
    "        print(\"Downloaded \" + local)\n",
    "\n",
    "\n",
    "download(\"https://github.com/AllenDowney/ThinkStats2/raw/master/code/thinkstats2.py\")\n",
    "download(\"https://github.com/AllenDowney/ThinkStats2/raw/master/code/thinkplot.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "955f8f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import thinkstats2\n",
    "import thinkplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a27c56",
   "metadata": {},
   "source": [
    "I'll start with the data from the BRFSS again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327c8d17",
   "metadata": {},
   "source": [
    "## PDFs\n",
    "\n",
    "The derivative of a CDF is called a **probability density function**, or\n",
    "PDF. For example, the PDF of an exponential distribution is\n",
    "$$PDF_{expo}(x) = \\lambda e^{-\\lambda x}$$ The PDF of a normal\n",
    "distribution is $$PDF_{normal}(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \n",
    "                 \\exp \\left[ -\\frac{1}{2} \n",
    "                 \\left( \\frac{x - \\mu}{\\sigma} \\right)^2 \\right]$$\n",
    "Evaluating a PDF for a particular value of $x$ is usually not useful.\n",
    "The result is not a probability; it is a probability *density*.\n",
    "\n",
    "In physics, density is mass per unit of volume; in order to get a mass,\n",
    "you have to multiply by volume or, if the density is not constant, you\n",
    "have to integrate over volume.\n",
    "\n",
    "Similarly, **probability density** measures probability per unit of $x$.\n",
    "In order to get a probability mass, you have to integrate over $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43aba55",
   "metadata": {},
   "source": [
    "`thinkstats2` provides a class called Pdf that represents a probability\n",
    "density function. Every Pdf object provides the following methods:\n",
    "\n",
    "-   `Density`, which takes a value, `x`, and returns the density of the\n",
    "    distribution at `x`.\n",
    "\n",
    "-   `Render`, which evaluates the density at a discrete set of values\n",
    "    and returns a pair of sequences: the sorted values, `xs`, and their\n",
    "    probability densities, `ds`.\n",
    "\n",
    "-   `MakePmf`, which evaluates `Density` at a discrete set of values and\n",
    "    returns a normalized Pmf that approximates the Pdf.\n",
    "\n",
    "-   `GetLinspace`, which returns the default set of points used by\n",
    "    `Render` and `MakePmf`.\n",
    "\n",
    "Pdf is an abstract parent class, which means you should not instantiate\n",
    "it; that is, you cannot create a Pdf object. Instead, you should define\n",
    "a child class that inherits from Pdf and provides definitions of\n",
    "`Density` and `GetLinspace`. Pdf provides `Render` and `MakePmf`.\n",
    "\n",
    "For example, `thinkstats2` provides a class named `NormalPdf` that\n",
    "evaluates the normal density function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a0f012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thinkstats2 import Pdf\n",
    "\n",
    "\n",
    "class NormalPdf(Pdf):\n",
    "\n",
    "    def __init__(self, mu=0, sigma=1, label=\"\"):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.label = label\n",
    "\n",
    "    def density(self, xs):\n",
    "        return scipy.stats.norm.pdf(xs, self.mu, self.sigma)\n",
    "\n",
    "    def get_linspace(self):\n",
    "        low, high = self.mu - 3 * self.sigma, self.mu + 3 * self.sigma\n",
    "        return np.linspace(low, high, 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956261d5",
   "metadata": {},
   "source": [
    "The NormalPdf object contains the parameters `mu` and `sigma`. `Density`\n",
    "uses `scipy.stats.norm`, which is an object that represents a normal\n",
    "distribution and provides `cdf` and `pdf`, among other methods (see\n",
    "Section [\\[normal\\]](#normal){reference-type=\"ref\" reference=\"normal\"}).\n",
    "\n",
    "The following example creates a NormalPdf with the mean and variance of\n",
    "adult female heights, in cm, from the BRFSS (see\n",
    "Section [\\[brfss\\]](#brfss){reference-type=\"ref\" reference=\"brfss\"}).\n",
    "Then it computes the density of the distribution at a location one\n",
    "standard deviation from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64ac3b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "download(\"https://github.com/AllenDowney/ThinkStats2/raw/master/code/brfss.py\")\n",
    "download(\"https://github.com/AllenDowney/ThinkStats2/raw/master/code/CDBRFS08.ASC.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e45150ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brfss\n",
    "\n",
    "df = brfss.read_brfss(nrows=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a4e85ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "female = df[df.sex == 2]\n",
    "female_heights = female.htm3.dropna()\n",
    "mean, std = female_heights.mean(), female_heights.std()\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "452138d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = thinkstats2.NormalPdf(mean, std)\n",
    "pdf.density(mean + std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c185111e",
   "metadata": {},
   "source": [
    "The result is about 0.03, in units of probability mass per cm. Again, a\n",
    "probability density doesn't mean much by itself. But if we plot the Pdf,\n",
    "we can see the shape of the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ddbf508",
   "metadata": {},
   "outputs": [],
   "source": [
    "thinkplot.pdf(pdf, label=\"normal\")\n",
    "thinkplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b758b289",
   "metadata": {},
   "source": [
    "`thinkplot.Pdf` plots the Pdf as a smooth function, as contrasted with\n",
    "`thinkplot.Pmf`, which renders a Pmf as a step function.\n",
    "Figure [\\[pdf_example\\]](#pdf_example){reference-type=\"ref\"\n",
    "reference=\"pdf_example\"} shows the result, as well as a PDF estimated\n",
    "from a sample, which we'll compute in the next section.\n",
    "\n",
    "You can use `MakePmf` to approximate the Pdf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f177588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmf = pdf.make_pmf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491f7694",
   "metadata": {},
   "source": [
    "By default, the resulting Pmf contains 101 points equally spaced from\n",
    "`mu - 3*sigma` to `mu + 3*sigma`. Optionally, `MakePmf` and `Render` can\n",
    "take keyword arguments `low`, `high`, and `n`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91858c9e",
   "metadata": {},
   "source": [
    "## Kernel density estimation\n",
    "\n",
    "**Kernel density estimation** (KDE) is an algorithm that takes a sample\n",
    "and finds an appropriately smooth PDF that fits the data. You can read\n",
    "details at <http://en.wikipedia.org/wiki/Kernel_density_estimation>.\n",
    "\n",
    "`scipy` provides an implementation of KDE and `thinkstats2` provides a\n",
    "class called `EstimatedPdf` that uses it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "492c2fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EstimatedPdf(Pdf):\n",
    "\n",
    "    def __init__(self, sample):\n",
    "        self.kde = scipy.stats.gaussian_kde(sample)\n",
    "\n",
    "    def density(self, xs):\n",
    "        return self.kde.evaluate(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac2ef96",
   "metadata": {},
   "source": [
    "`__init__` takes a sample and computes a kernel density estimate. The\n",
    "result is a `gaussian_kde` object that provides an `evaluate` method.\n",
    "\n",
    "`Density` takes a value or sequence, calls `gaussian_kde.evaluate`, and\n",
    "returns the resulting density. The word \"Gaussian\" appears in the name\n",
    "because it uses a filter based on a Gaussian distribution to smooth the\n",
    "KDE.\n",
    "\n",
    "Here's an example that generates a sample from a normal distribution and\n",
    "then makes an EstimatedPdf to fit it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "363b8193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sample = [random.gauss(mean, std) for i in range(500)]\n",
    "sample_pdf = thinkstats2.EstimatedPdf(sample)\n",
    "thinkplot.pdf(sample_pdf, label=\"sample KDE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547b06b6",
   "metadata": {},
   "source": [
    "`sample` is a list of 500 random heights. `sample_pdf` is a Pdf object\n",
    "that contains the estimated KDE of the sample.\n",
    "\n",
    "Figure [\\[pdf_example\\]](#pdf_example){reference-type=\"ref\"\n",
    "reference=\"pdf_example\"} shows the normal density function and a KDE\n",
    "based on a sample of 500 random heights. The estimate is a good match\n",
    "for the original distribution.\n",
    "\n",
    "Estimating a density function with KDE is useful for several purposes:\n",
    "\n",
    "-   *Visualization:* During the exploration phase of a project, CDFs are\n",
    "    usually the best visualization of a distribution. After you look at\n",
    "    a CDF, you can decide whether an estimated PDF is an appropriate\n",
    "    model of the distribution. If so, it can be a better choice for\n",
    "    presenting the distribution to an audience that is unfamiliar with\n",
    "    CDFs.\n",
    "\n",
    "-   *Interpolation:* An estimated PDF is a way to get from a sample to a\n",
    "    model of the population. If you have reason to believe that the\n",
    "    population distribution is smooth, you can use KDE to interpolate\n",
    "    the density for values that don't appear in the sample.\n",
    "\n",
    "-   *Simulation:* Simulations are often based on the distribution of a\n",
    "    sample. If the sample size is small, it might be appropriate to\n",
    "    smooth the sample distribution using KDE, which allows the\n",
    "    simulation to explore more possible outcomes, rather than\n",
    "    replicating the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb6bc7c",
   "metadata": {},
   "source": [
    "## The distribution framework\n",
    "\n",
    "![A framework that relates representations of distribution\n",
    "functions.](figs/distribution_functions.pdf){height=\"2.2in\"}\n",
    "\n",
    "At this point we have seen PMFs, CDFs and PDFs; let's take a minute to\n",
    "review.\n",
    "Figure [\\[dist_framework\\]](#dist_framework){reference-type=\"ref\"\n",
    "reference=\"dist_framework\"} shows how these functions relate to each\n",
    "other.\n",
    "\n",
    "We started with PMFs, which represent the probabilities for a discrete\n",
    "set of values. To get from a PMF to a CDF, you add up the probability\n",
    "masses to get cumulative probabilities. To get from a CDF back to a PMF,\n",
    "you compute differences in cumulative probabilities. We'll see the\n",
    "implementation of these operations in the next few sections.\n",
    "\n",
    "A PDF is the derivative of a continuous CDF; or, equivalently, a CDF is\n",
    "the integral of a PDF. Remember that a PDF maps from values to\n",
    "probability densities; to get a probability, you have to integrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a82910f",
   "metadata": {},
   "source": [
    "To get from a discrete to a continuous distribution, you can perform\n",
    "various kinds of smoothing. One form of smoothing is to assume that the\n",
    "data come from an analytic continuous distribution (like exponential or\n",
    "normal) and to estimate the parameters of that distribution. Another\n",
    "option is kernel density estimation.\n",
    "\n",
    "The opposite of smoothing is **discretizing**, or quantizing. If you\n",
    "evaluate a PDF at discrete points, you can generate a PMF that is an\n",
    "approximation of the PDF. You can get a better approximation using\n",
    "numerical integration.\n",
    "\n",
    "To distinguish between continuous and discrete CDFs, it might be better\n",
    "for a discrete CDF to be a \"cumulative mass function,\" but as far as I\n",
    "can tell no one uses that term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0018d7",
   "metadata": {},
   "source": [
    "## Hist implementation\n",
    "\n",
    "At this point you should know how to use the basic types provided by\n",
    "`thinkstats2`: Hist, Pmf, Cdf, and Pdf. The next few sections provide\n",
    "details about how they are implemented. This material might help you use\n",
    "these classes more effectively, but it is not strictly necessary.\n",
    "\n",
    "Hist and Pmf inherit from a parent class called `_DictWrapper`. The\n",
    "leading underscore indicates that this class is \"internal;\" that is, it\n",
    "should not be used by code in other modules. The name indicates what it\n",
    "is: a dictionary wrapper. Its primary attribute is `d`, the dictionary\n",
    "that maps from values to their frequencies.\n",
    "\n",
    "The values can be any hashable type. The frequencies should be integers,\n",
    "but can be any numeric type.\n",
    "\n",
    "`_DictWrapper` contains methods appropriate for both Hist and Pmf,\n",
    "including `__init__`, `Values`, `Items` and `Render`. It also provides\n",
    "modifier methods `Set`, `Incr`, `Mult`, and `Remove`. These methods are\n",
    "all implemented with dictionary operations. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "672faf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def incr(self, x, term=1):\n",
    "    self.d[x] = self.d.get(x, 0) + term\n",
    "\n",
    "\n",
    "def mult(self, x, factor):\n",
    "    self.d[x] = self.d.get(x, 0) * factor\n",
    "\n",
    "\n",
    "def remove(self, x):\n",
    "    del self.d[x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076b8ce6",
   "metadata": {},
   "source": [
    "Hist also provides `Freq`, which looks up the frequency of a given\n",
    "value.\n",
    "\n",
    "Because Hist operators and methods are based on dictionaries, these\n",
    "methods are constant time operations; that is, their run time does not\n",
    "increase as the Hist gets bigger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5fae95",
   "metadata": {},
   "source": [
    "## Pmf implementation\n",
    "\n",
    "Pmf and Hist are almost the same thing, except that a Pmf maps values to\n",
    "floating-point probabilities, rather than integer frequencies. If the\n",
    "sum of the probabilities is 1, the Pmf is normalized.\n",
    "\n",
    "Pmf provides `Normalize`, which computes the sum of the probabilities\n",
    "and divides through by a factor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3881b291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(self, fraction=1.0):\n",
    "    total = self.total()\n",
    "    if total == 0.0:\n",
    "        raise ValueError(\"Total probability is zero.\")\n",
    "    factor = float(fraction) / total\n",
    "    for x in self.d:\n",
    "        self.d[x] *= factor\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470d1825",
   "metadata": {},
   "source": [
    "`fraction` determines the sum of the probabilities after normalizing;\n",
    "the default value is 1. If the total probability is 0, the Pmf cannot be\n",
    "normalized, so `Normalize` raises `ValueError`.\n",
    "\n",
    "Hist and Pmf have the same constructor. It can take as an argument a\n",
    "`dict`, Hist, Pmf or Cdf, a pandas Series, a list of (value, frequency)\n",
    "pairs, or a sequence of values.\n",
    "\n",
    "If you instantiate a Pmf, the result is normalized. If you instantiate a\n",
    "Hist, it is not. To construct an unnormalized Pmf, you can create an\n",
    "empty Pmf and modify it. The Pmf modifiers do not renormalize the Pmf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced7ce0c",
   "metadata": {},
   "source": [
    "## Cdf implementation\n",
    "\n",
    "A CDF maps from values to cumulative probabilities, so I could have\n",
    "implemented Cdf as a `_DictWrapper`. But the values in a CDF are ordered\n",
    "and the values in a `_DictWrapper` are not. Also, it is often useful to\n",
    "compute the inverse CDF; that is, the map from cumulative probability to\n",
    "value. So the implementaion I chose is two sorted lists. That way I can\n",
    "use binary search to do a forward or inverse lookup in logarithmic time.\n",
    "\n",
    "The Cdf constructor can take as a parameter a sequence of values or a\n",
    "pandas Series, a dictionary that maps from values to probabilities, a\n",
    "sequence of (value, probability) pairs, a Hist, Pmf, or Cdf. Or if it is\n",
    "given two parameters, it treats them as a sorted sequence of values and\n",
    "the sequence of corresponding cumulative probabilities.\n",
    "\n",
    "Given a sequence, pandas Series, or dictionary, the constructor makes a\n",
    "Hist. Then it uses the Hist to initialize the attributes:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83a33778",
   "metadata": {},
   "source": [
    "self.xs, freqs = zip(*sorted(dw.Items()))\n",
    "self.ps = np.cumsum(freqs, dtype=np.float)\n",
    "self.ps /= self.ps[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59809f47",
   "metadata": {},
   "source": [
    "`xs` is the sorted list of values; `freqs` is the list of corresponding\n",
    "frequencies. `np.cumsum` computes the cumulative sum of the frequencies.\n",
    "Dividing through by the total frequency yields cumulative probabilities.\n",
    "For `n` values, the time to construct the Cdf is proportional to\n",
    "$n \\log n$.\n",
    "\n",
    "Here is the implementation of `Prob`, which takes a value and returns\n",
    "its cumulative probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17bd7f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(self, x):\n",
    "    if x < self.xs[0]:\n",
    "        return 0.0\n",
    "    index = bisect.bisect(self.xs, x)\n",
    "    p = self.ps[index - 1]\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879a57d0",
   "metadata": {},
   "source": [
    "The `bisect` module provides an implementation of binary search. And\n",
    "here is the implementation of `Value`, which takes a cumulative\n",
    "probability and returns the corresponding value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aad82744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value(self, p):\n",
    "    if p < 0 or p > 1:\n",
    "        raise ValueError(\"p must be in range [0, 1]\")\n",
    "    index = bisect.bisect_left(self.ps, p)\n",
    "    return self.xs[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67ea490",
   "metadata": {},
   "source": [
    "Given a Cdf, we can compute the Pmf by computing differences between\n",
    "consecutive cumulative probabilities. If you call the Cdf constructor\n",
    "and pass a Pmf, it computes differences by calling `Cdf.Items`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b5f6fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def items(self):\n",
    "    a = self.ps\n",
    "    b = np.roll(a, 1)\n",
    "    b[0] = 0\n",
    "    return zip(self.xs, a - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcdda40",
   "metadata": {},
   "source": [
    "`np.roll` shifts the elements of `a` to the right, and \"rolls\" the last\n",
    "one back to the beginning. We replace the first element of `b` with 0\n",
    "and then compute the difference `a-b`. The result is a NumPy array of\n",
    "probabilities.\n",
    "\n",
    "Cdf provides `Shift` and `Scale`, which modify the values in the Cdf,\n",
    "but the probabilities should be treated as immutable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d8c58e",
   "metadata": {},
   "source": [
    "## Moments\n",
    "\n",
    "Any time you take a sample and reduce it to a single number, that number\n",
    "is a statistic. The statistics we have seen so far include mean,\n",
    "variance, median, and interquartile range.\n",
    "\n",
    "A **raw moment** is a kind of statistic. If you have a sample of values,\n",
    "$x_i$, the $k$th raw moment is: $$m'_k = \\frac{1}{n} \\sum_i x_i^k$$ Or\n",
    "if you prefer Python notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4247231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_moment(xs, k):\n",
    "    return sum(x**k for x in xs) / len(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "340e35a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(xs):\n",
    "    return raw_moment(xs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb23e825",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(female_heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd7d0b",
   "metadata": {},
   "source": [
    "When $k=1$ the result is the sample mean, $\\bar{x}$. The other raw moments\n",
    "don't mean much by themselves, but they are used in some computations.\n",
    "\n",
    "The **central moments** are more useful. The $k$th central moment is:\n",
    "$$m_k = \\frac{1}{n} \\sum_i (x_i - \\bar{x})^k$$ Or in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfb7012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def central_moment(xs, k):\n",
    "    mean = raw_moment(xs, 1)\n",
    "    return sum((x - mean) ** k for x in xs) / len(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61926432",
   "metadata": {},
   "outputs": [],
   "source": [
    "central_moment(female_heights, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4510bad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "central_moment(female_heights, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fc44e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "central_moment(female_heights, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095c40c1",
   "metadata": {},
   "source": [
    "When $k=2$ the result is the second central moment, which you might\n",
    "recognize as variance. The definition of variance gives a hint about why\n",
    "these statistics are called moments. If we attach a weight along a ruler\n",
    "at each location, $x_i$, and then spin the ruler around the mean, the\n",
    "moment of inertia of the spinning weights is the variance of the values.\n",
    "If you are not familiar with moment of inertia, see\n",
    "<http://en.wikipedia.org/wiki/Moment_of_inertia>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12147739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def var(xs):\n",
    "    return central_moment(xs, 2)\n",
    "\n",
    "\n",
    "var(female_heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f97f91",
   "metadata": {},
   "source": [
    "When you report moment-based statistics, it is important to think about\n",
    "the units. For example, if the values $x_i$ are in cm, the first raw\n",
    "moment is also in cm. But the second moment is in cm$^2$, the third\n",
    "moment is in cm$^3$, and so on.\n",
    "\n",
    "Because of these units, moments are hard to interpret by themselves.\n",
    "That's why, for the second moment, it is common to report standard\n",
    "deviation, which is the square root of variance, so it is in the same\n",
    "units as $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c926907",
   "metadata": {},
   "source": [
    "## Skewness\n",
    "\n",
    "**Skewness** is a property that describes the shape of a distribution.\n",
    "If the distribution is symmetric around its central tendency, it is\n",
    "unskewed. If the values extend farther to the right, it is \"right\n",
    "skewed\" and if the values extend left, it is \"left skewed.\"\n",
    "\n",
    "This use of \"skewed\" does not have the usual connotation of \"biased.\"\n",
    "Skewness only describes the shape of the distribution; it says nothing\n",
    "about whether the sampling process might have been biased.\n",
    "\n",
    "Several statistics are commonly used to quantify the skewness of a\n",
    "distribution. Given a sequence of values, $x_i$, the **sample\n",
    "skewness**, $g_1$, can be computed like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a1d93df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardized_moment(xs, k):\n",
    "    var = central_moment(xs, 2)\n",
    "    std = np.sqrt(var)\n",
    "    return central_moment(xs, k) / std**k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b8a6c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_moment(female_heights, 1), standardized_moment(\n",
    "    female_heights, 2\n",
    "), standardized_moment(female_heights, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0268216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skewness(xs):\n",
    "    return standardized_moment(xs, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4dfacd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness(female_heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4592341",
   "metadata": {},
   "source": [
    "$g_1$ is the third **standardized moment**, which means that it has been\n",
    "normalized so it has no units.\n",
    "\n",
    "Negative skewness indicates that a distribution skews left; positive\n",
    "skewness indicates that a distribution skews right. The magnitude of\n",
    "$g_1$ indicates the strength of the skewness, but by itself it is not\n",
    "easy to interpret.\n",
    "\n",
    "In practice, computing sample skewness is usually not a good idea. If\n",
    "there are any outliers, they have a disproportionate effect on $g_1$.\n",
    "\n",
    "Another way to evaluate the asymmetry of a distribution is to look at\n",
    "the relationship between the mean and median. Extreme values have more\n",
    "effect on the mean than the median, so in a distribution that skews\n",
    "left, the mean is less than the median. In a distribution that skews\n",
    "right, the mean is greater.\n",
    "\n",
    "**Pearson's median skewness coefficient** is a measure of skewness based\n",
    "on the difference between the sample mean and median:\n",
    "$$g_p = 3 (\\bar{x} - m) / S$$ Where $\\bar{x}$ is the sample mean, $m$ is the\n",
    "median, and $S$ is the standard deviation. Or in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "05b8b6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def median(xs):\n",
    "    cdf = thinkstats2.Cdf(xs)\n",
    "    return cdf.value(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc62c23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(female_heights), median(female_heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "78c5f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_median_skewness(xs):\n",
    "    xbar = raw_moment(xs, 1)\n",
    "    m = median(xs)\n",
    "    s2 = central_moment(xs, 2)\n",
    "    s = np.sqrt(s2)\n",
    "    gp = 3 * (xbar - m) / s\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed0887cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_median_skewness(female_heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3ca4ec",
   "metadata": {},
   "source": [
    "This statistic is **robust**, which means that it is less vulnerable to\n",
    "the effect of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af2e5656",
   "metadata": {},
   "outputs": [],
   "source": [
    "download(\"https://github.com/AllenDowney/ThinkStats2/raw/master/code/nsfg.py\")\n",
    "download(\"https://github.com/AllenDowney/ThinkStats2/raw/master/code/2002FemPreg.dct\")\n",
    "download(\n",
    "    \"https://github.com/AllenDowney/ThinkStats2/raw/master/code/2002FemPreg.dat.gz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaed2aa7",
   "metadata": {},
   "source": [
    "As an example, let's look at the skewness of birth weights in the NSFG\n",
    "pregnancy data. Here's the code to estimate and plot the PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "30b28e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nsfg\n",
    "\n",
    "live, firsts, others = nsfg.make_frames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e7cbbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = live.totalwgt_lb.dropna()\n",
    "pdf = thinkstats2.EstimatedPdf(data)\n",
    "thinkplot.pdf(pdf, label=\"birth weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8111241a",
   "metadata": {},
   "source": [
    "Figure [\\[density_totalwgt_kde\\]](#density_totalwgt_kde){reference-type=\"ref\"\n",
    "reference=\"density_totalwgt_kde\"} shows the result. The left tail\n",
    "appears longer than the right, so we suspect the distribution is skewed\n",
    "left. The mean, 7.27 lbs, is a bit less than the median, 7.38 lbs, so\n",
    "that is consistent with left skew. And both skewness coefficients are\n",
    "negative: sample skewness is -0.59; Pearson's median skewness is -0.23."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925530bc",
   "metadata": {},
   "source": [
    "![Estimated PDF of adult weight data from the\n",
    "BRFSS.](figs/density_wtkg2_kde.pdf){height=\"2.2in\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5c5621",
   "metadata": {},
   "source": [
    "Now let's compare this distribution to the distribution of adult weight\n",
    "in the BRFSS. Again, here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8e124a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = brfss.read_brfss(nrows=None)\n",
    "data = df.wtkg2.dropna()\n",
    "pdf = thinkstats2.EstimatedPdf(data)\n",
    "thinkplot.pdf(pdf, label=\"adult weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46267ba",
   "metadata": {},
   "source": [
    "Figure [\\[density_wtkg2_kde\\]](#density_wtkg2_kde){reference-type=\"ref\"\n",
    "reference=\"density_wtkg2_kde\"} shows the result. The distribution\n",
    "appears skewed to the right. Sure enough, the mean, 79.0, is bigger than\n",
    "the median, 77.3. The sample skewness is 1.1 and Pearson's median\n",
    "skewness is 0.26.\n",
    "\n",
    "The sign of the skewness coefficient indicates whether the distribution\n",
    "skews left or right, but other than that, they are hard to interpret.\n",
    "Sample skewness is less robust; that is, it is more susceptible to\n",
    "outliers. As a result it is less reliable when applied to skewed\n",
    "distributions, exactly when it would be most relevant.\n",
    "\n",
    "Pearson's median skewness is based on a computed mean and variance, so\n",
    "it is also susceptible to outliers, but since it does not depend on a\n",
    "third moment, it is somewhat more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d0b0db",
   "metadata": {},
   "source": [
    "## Glossary\n",
    "\n",
    "-   **Probability density function (PDF)**: The derivative of a\n",
    "    continuous CDF, a function that maps a value to its probability\n",
    "    density.\n",
    "\n",
    "-   **Probability density**: A quantity that can be integrated over a\n",
    "    range of values to yield a probability. If the values are in units\n",
    "    of cm, for example, probability density is in units of probability\n",
    "    per cm.\n",
    "\n",
    "-   **Kernel density estimation (KDE)**: An algorithm that estimates a\n",
    "    PDF based on a sample.\n",
    "\n",
    "-   **discretize**: To approximate a continuous function or distribution\n",
    "    with a discrete function. The opposite of smoothing.\n",
    "\n",
    "-   **raw moment**: A statistic based on the sum of data raised to a\n",
    "    power.\n",
    "\n",
    "-   **central moment**: A statistic based on deviation from the mean,\n",
    "    raised to a power.\n",
    "\n",
    "-   **standardized moment**: A ratio of moments that has no units.\n",
    "\n",
    "-   **skewness**: A measure of how asymmetric a distribution is.\n",
    "\n",
    "-   **sample skewness**: A moment-based statistic intended to quantify\n",
    "    the skewness of a distribution.\n",
    "\n",
    "-   **Pearson's median skewness coefficient**: A statistic intended to\n",
    "    quantify the skewness of a distribution based on the median, mean,\n",
    "    and standard deviation.\n",
    "\n",
    "-   **robust**: A statistic is robust if it is relatively immune to the\n",
    "    effect of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21248f4",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c5d940",
   "metadata": {},
   "source": [
    "The distribution of income is famously skewed to the right. In this exercise, we’ll measure how strong that skew is.\n",
    "The Current Population Survey (CPS) is a joint effort of the Bureau of Labor Statistics and the Census Bureau to study income and related variables. Data collected in 2013 is available from http://www.census.gov/hhes/www/cpstables/032013/hhinc/toc.htm. I downloaded `hinc06.xls`, which is an Excel spreadsheet with information about household income, and converted it to `hinc06.csv`, a CSV file you will find in the repository for this book. You will also find `hinc2.py`, which reads this file and transforms the data.\n",
    "\n",
    "The dataset is in the form of a series of income ranges and the number of respondents who fell in each range. The lowest range includes respondents who reported annual household income “Under \\$5000.” \n",
    "\n",
    "The highest range includes respondents who made “\\$250,000 or more.”\n",
    "\n",
    "To estimate mean and other statistics from these data, we have to make some assumptions about the lower and upper bounds, and how the values are distributed in each range. `hinc2.py` provides `InterpolateSample`, which shows one way to model this data. It takes a `DataFrame` with a column, `income`, that contains the upper bound of each range, and `freq`, which contains the number of respondents in each frame.\n",
    "\n",
    "It also takes `log_upper`, which is an assumed upper bound on the highest range, expressed in `log10` dollars. The default value, `log_upper=6.0` represents the assumption that the largest income among the respondents is $10^6$, or one million dollars.\n",
    "\n",
    "`InterpolateSample` generates a pseudo-sample; that is, a sample of household incomes that yields the same number of respondents in each range as the actual data. It assumes that incomes in each range are equally spaced on a `log10` scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "39fe25c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_sample(df, log_upper=6.0):\n",
    "    \"\"\"Makes a sample of log10 household income.\n",
    "\n",
    "    Assumes that log10 income is uniform in each range.\n",
    "\n",
    "    df: DataFrame with columns income and freq\n",
    "    log_upper: log10 of the assumed upper bound for the highest range\n",
    "\n",
    "    returns: NumPy array of log10 household income\n",
    "    \"\"\"\n",
    "    df[\"log_upper\"] = np.log10(df.income)\n",
    "    df[\"log_lower\"] = df.log_upper.shift(1)\n",
    "    df.loc[0, \"log_lower\"] = 3.0\n",
    "    df.loc[41, \"log_upper\"] = log_upper\n",
    "    arrays = []\n",
    "    for _, row in df.iterrows():\n",
    "        vals = np.linspace(row.log_lower, row.log_upper, int(row.freq))\n",
    "        arrays.append(vals)\n",
    "    log_sample = np.concatenate(arrays)\n",
    "    return log_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "762e66be",
   "metadata": {},
   "outputs": [],
   "source": [
    "download(\"https://github.com/AllenDowney/ThinkStats2/raw/master/code/hinc.py\")\n",
    "download(\"https://github.com/AllenDowney/ThinkStats2/raw/master/code/hinc06.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d738aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hinc\n",
    "\n",
    "income_df = hinc.read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "004dd414",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_sample = interpolate_sample(income_df, log_upper=6.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8c0f5a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_cdf = thinkstats2.Cdf(log_sample)\n",
    "thinkplot.cdf(log_cdf)\n",
    "thinkplot.config(xlabel=\"Household income (log $)\", ylabel=\"CDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "127d4383",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.power(10, log_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f49ad492",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = thinkstats2.Cdf(sample)\n",
    "thinkplot.cdf(cdf)\n",
    "thinkplot.config(xlabel=\"Household income ($)\", ylabel=\"CDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25f96e4",
   "metadata": {},
   "source": [
    "Compute the median, mean, skewness and Pearson’s skewness of the resulting sample. What fraction of households report a taxable income below the mean? How do the results depend on the assumed upper bound?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7f2b2719",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(sample), median(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b771f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness(sample), pearson_median_skewness(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e849e80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.prob(mean(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e5bd15",
   "metadata": {},
   "source": [
    "All of this is based on an assumption that the highest income is one million dollars, but that's certainly not correct.  What happens to the skew if the upper bound is 10 million?\n",
    "\n",
    "Without better information about the top of this distribution, we can't say much about the skewness of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c726d530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
