{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07d4536a",
   "metadata": {},
   "source": [
    "# Time series analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afc7a155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import basename, exists\n",
    "\n",
    "\n",
    "def download(url):\n",
    "    filename = basename(url)\n",
    "    if not exists(filename):\n",
    "        from urllib.request import urlretrieve\n",
    "\n",
    "        local, _ = urlretrieve(url, filename)\n",
    "        print(\"Downloaded \" + local)\n",
    "\n",
    "\n",
    "download(\"https://github.com/AllenDowney/ThinkStats/raw/v3/nb/thinkstats.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0005d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from thinkstats import decorate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714edc07",
   "metadata": {},
   "source": [
    "A **time series** is a sequence of measurements from a system that varies in time.\n",
    "One famous example is the \"hockey stick graph\" that shows global average temperature over time (see <https://en.wikipedia.org/wiki/Hockey_stick_graph>).\n",
    "\n",
    "The example I work with in this chapter comes from Zachary M. Jones, a researcher in political science who studies the black market for cannabis in the U.S. (<http://zmjones.com/marijuana>).\n",
    "He collected data from a web site called \"Price of Weed\" that crowdsources market information by asking participants to report the price, quantity, quality, and location of cannabis transactions (<http://www.priceofweed.com/>).\n",
    "The goal of his project is to investigate the effect of policy decisions, like legalization, on markets.\n",
    "I find this project appealing because it is an example that uses data to address important political questions, like drug policy.\n",
    "\n",
    "I hope you will find this chapter interesting, but I'll take this opportunity to reiterate the importance of maintaining a professional attitude to data analysis.\n",
    "Whether and which drugs should be illegal are important and difficult public policy questions; our decisions should be informed by accurate data reported honestly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0aeec3",
   "metadata": {},
   "source": [
    "## Importing and cleaning\n",
    "\n",
    "The data I downloaded from Mr. Jones's site is in the repository for this book.\n",
    "The following code reads it into a Pandas `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e1cf2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "download(\"https://github.com/AllenDowney/ThinkStats/raw/v3/data/mj-clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a836b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "transactions = pd.read_csv(\"mj-clean.csv\", parse_dates=[5])\n",
    "transactions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbac797c",
   "metadata": {},
   "source": [
    "`parse_dates` tells `read_csv` to interpret values in column 5 as dates and convert them to NumPy `datetime64` objects.\n",
    "\n",
    "The `DataFrame` has a row for each reported transaction and the following columns:\n",
    "\n",
    "-   city: string city name.\n",
    "\n",
    "-   state: two-letter state abbreviation.\n",
    "\n",
    "-   price: price paid in dollars.\n",
    "\n",
    "-   amount: quantity purchased in grams.\n",
    "\n",
    "-   quality: high, medium, or low quality, as reported by the purchaser.\n",
    "\n",
    "-   date: date of report, presumed to be shortly after date of purchase.\n",
    "\n",
    "-   ppg: price per gram, in dollars.\n",
    "\n",
    "-   state.name: string state name.\n",
    "\n",
    "-   lat: approximate latitude of the transaction, based on city name.\n",
    "\n",
    "-   lon: approximate longitude of the transaction.\n",
    "\n",
    "Each transaction is an event in time, so we could treat this dataset as a time series.\n",
    "But the events are not equally spaced in time; the number of transactions reported each day varies from 0 to several hundred.\n",
    "Many methods used to analyze time series require the measurements to be equally spaced, or at least things are simpler if they are.\n",
    "\n",
    "In order to demonstrate these methods, I divide the dataset into groups by reported quality, and then transform each group into an equally spaced series by computing the mean daily price per gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b783179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_quality_and_day(transactions):\n",
    "    \"\"\"Divides transactions by quality and computes mean daily price.\n",
    "\n",
    "    transaction: DataFrame of transactions\n",
    "\n",
    "    returns: map from quality to time series of ppg\n",
    "    \"\"\"\n",
    "    groups = transactions.groupby(\"quality\")\n",
    "    dailies = {}\n",
    "    for name, group in groups:\n",
    "        dailies[name] = group_by_day(group)\n",
    "    return dailies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c914a58",
   "metadata": {},
   "source": [
    "`groupby` is a `DataFrame` method that returns a GroupBy object, `groups`; used in a for loop, it iterates the names of the groups and the `DataFrame`s that represent them.\n",
    "Since the values of `quality` are `low`, `medium`, and `high`, we get three groups with those names.\n",
    "\n",
    "The loop iterates through the groups and calls `GroupByDay`, which computes the daily average price and returns a new `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "333a65ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_day(transactions, func=\"mean\"):\n",
    "    \"\"\"Groups transactions by day and compute the daily mean ppg.\n",
    "\n",
    "    transactions: DataFrame of transactions\n",
    "\n",
    "    returns: DataFrame of daily prices\n",
    "    \"\"\"\n",
    "    grouped = transactions[[\"date\", \"ppg\"]].groupby(\"date\")\n",
    "    daily = grouped.aggregate(func)\n",
    "    daily[\"date\"] = daily.index\n",
    "    start = daily.date.iloc[0]\n",
    "    one_year = np.timedelta64(52, \"W\")\n",
    "    daily[\"years\"] = (daily.date - start) / one_year\n",
    "    return daily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cccbd8",
   "metadata": {},
   "source": [
    "The parameter, `transactions`, is a `DataFrame` that contains columns `date` and `ppg`.\n",
    "We select these two columns, then group by `date`.\n",
    "\n",
    "The result, `grouped`, is a map from each date to a `DataFrame` that contains prices reported on that date.\n",
    "`aggregate` is a GroupBy method that iterates through the groups and applies a function to each column of the group; in this case there is only one column, `ppg`.\n",
    "So the result of `aggregate` is a `DataFrame` with one row for each date and one column, `ppg`.\n",
    "\n",
    "Dates in these `DataFrame`s are stored as NumPy `datetime64` objects, which are represented as 64-bit integers in nanoseconds.\n",
    "For some of the analyses coming up, it will be convenient to work with time in more human-friendly units, like years.\n",
    "So `GroupByDay` adds a column named `date` by copying the `index`, then adds `years`, which contains the number of years since the first transaction as a floating-point number.\n",
    "\n",
    "The resulting `DataFrame` has columns `ppg`, `date`, and `years`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6006cbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "dailies = group_by_quality_and_day(transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0b6fa2",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "The result from `GroupByQualityAndDay` is a map from each quality to a `DataFrame` of daily prices.\n",
    "Here's the code I use to plot the three time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf58da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, (name, daily) in enumerate(dailies.items()):\n",
    "    plt.subplot(3, 1, i + 1)\n",
    "    title = \"Price per gram ($)\" if i == 0 else \"\"\n",
    "    plt.scatter(daily.index, daily.ppg, s=10, label=name)\n",
    "    decorate(ylim=[0, 20], title=title)\n",
    "\n",
    "    if i == 2:\n",
    "        plt.xticks(rotation=30)\n",
    "    else:\n",
    "        plt.xticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a03bf6e",
   "metadata": {},
   "source": [
    "The loop iterates through the `DataFrame`s and creates a scatter plot for each.\n",
    "It is common to plot time series with line segments between the points, but in this case there are many data points and prices are highly variable, so adding lines would not help.\n",
    "\n",
    "Since the labels on the x-axis are dates, I use `plt.xticks` to rotate the \"ticks\" 30 degrees, making them more readable.\n",
    "\n",
    "One apparent feature in these plots is a gap around November 2013.\n",
    "It's possible that data collection was not active during this time, or the data might not be available.\n",
    "We will consider ways to deal with this missing data later.\n",
    "\n",
    "Visually, it looks like the price of high quality cannabis is declining during this period, and the price of medium quality is increasing.\n",
    "The price of low quality might also be increasing, but it is harder to tell, since it seems to be more volatile.\n",
    "Keep in mind that quality data is reported by volunteers, so trends over time might reflect changes in how participants apply these labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c34317",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "Although there are methods specific to time series analysis, for many problems a simple way to get started is by applying general-purpose tools like linear regression.\n",
    "The following function takes a `DataFrame` of daily prices and computes a least squares fit, returning the model and results objects from StatsModels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b1f8eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "def run_linear_model(daily):\n",
    "    model = smf.ols(\"ppg ~ years\", data=daily)\n",
    "    results = model.fit()\n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c708b932",
   "metadata": {},
   "source": [
    "Then we can iterate through the qualities and fit a model to each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "548cbc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thinkstats import summarize_results\n",
    "\n",
    "for name, daily in dailies.items():\n",
    "    model, results = run_linear_model(daily)\n",
    "    print(name)\n",
    "    summarize_results(results)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3e1110",
   "metadata": {},
   "source": [
    "The estimated slopes indicate that the price of high quality cannabis dropped by about 71 cents per year during the observed interval; for medium quality it increased by 28 cents per year, and for low quality it increased by 57 cents per year.\n",
    "These estimates are all statistically significant with very small p-values.\n",
    "\n",
    "The $R^2$ value for high quality cannabis is 0.44, which means that time as an explanatory variable accounts for 44% of the observed variability in price.\n",
    "For the other qualities, the change in price is smaller, and variability in prices is higher, so the values of $R^2$ are smaller (but still statistically significant).\n",
    "\n",
    "The following code plots the observed prices and the fitted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4980d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fitted_values(model, results, label=\"\"):\n",
    "    years = model.exog[:, 1]\n",
    "    values = model.endog\n",
    "    plt.scatter(years, values, s=15, label=label)\n",
    "    plt.plot(years, results.fittedvalues, color=\"C1\", label=\"model\")\n",
    "    decorate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cd2ee7",
   "metadata": {},
   "source": [
    "From `model`, `plot_fitted_values` gets `exog` and `endog` , NumPy arrays with the exogenous (explanatory) and endogenous (dependent) variables.\n",
    "Then it makes a scatter plot of the data points and a line plot of the fitted values.\n",
    "\n",
    "The following figure shows the results for high quality cannabis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7c9eabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, results = run_linear_model(dailies[\"high\"])\n",
    "plot_fitted_values(model, results, label=\"high\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13770b2f",
   "metadata": {},
   "source": [
    "The model seems like a good linear fit for the data; nevertheless, linear regression is not the most appropriate choice for this data:\n",
    "\n",
    "-   First, there is no reason to expect the long-term trend to be a line or any other simple function.\n",
    "In general, prices are determined by supply and demand, both of which vary over time in unpredictable ways.\n",
    "\n",
    "-   Second, the linear regression model gives equal weight to all data, recent and past.\n",
    "For purposes of prediction, we should probably give more weight to recent data.\n",
    "\n",
    "-   Finally, one of the assumptions of linear regression is that the residuals are uncorrelated noise.\n",
    "With time series data, this assumption is often false because successive values are correlated.\n",
    "\n",
    "The next section presents an alternative that is more appropriate for time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb5896c",
   "metadata": {},
   "source": [
    "## Moving averages\n",
    "\n",
    "Most time series analysis is based on the modeling assumption that the observed series is the sum of three components:\n",
    "\n",
    "-   Trend: A smooth function that captures persistent changes.\n",
    "\n",
    "-   Seasonality: Periodic variation, possibly including daily, weekly, monthly, or yearly cycles.\n",
    "\n",
    "-   Noise: Random variation around the long-term trend.\n",
    "\n",
    "Regression is one way to extract the trend from a series, as we saw in the previous section.\n",
    "But if the trend is not a simple function, a good alternative is a **moving average**.\n",
    "A moving average divides the series into overlapping regions, called **windows**, and computes the average of the values in each window.\n",
    "\n",
    "One of the simplest moving averages is the **rolling mean**, which computes the mean of the values in each window.\n",
    "For example, if the window size is 3, the rolling mean computes the mean of values 0 through 2, 1 through 3, 2 through 4, etc.\n",
    "\n",
    "Pandas provides `rolling_mean`, which takes a `Series` and a window size and returns a new `Series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bede039",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9468aa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = pd.Series(array)\n",
    "series.rolling(3).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7861aa",
   "metadata": {},
   "source": [
    "The first two values are `nan`; the next value is the mean of the first three elements, 0, 1, and 2. The next value is the mean of 1, 2, and 3. And so on.\n",
    "\n",
    "Before we can apply `rolling_mean` to the cannabis data, we have to deal with missing values.\n",
    "There are a few days in the observed interval with no reported transactions for one or more quality categories, and a period in 2013 when data collection was not active.\n",
    "\n",
    "In the `DataFrame`s we have used so far, these dates are absent; the index skips days with no data.\n",
    "For the analysis that follows, we need to represent this missing data explicitly.\n",
    "We can do that by \"reindexing\" the `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "479a542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range(daily.index.min(), daily.index.max())\n",
    "reindexed = daily.reindex(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44253f9d",
   "metadata": {},
   "source": [
    "The first line computes a date range that includes every day from the beginning to the end of the observed interval.\n",
    "The second line creates a new `DataFrame` with all of the data from `daily`, but including rows for all dates, filled with `nan`.\n",
    "\n",
    "Now we can plot the rolling mean like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7553e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thinkstats import underride\n",
    "\n",
    "\n",
    "def scatter_series(xs, **options):\n",
    "    \"\"\"Makes a scatter plot from a Series.\n",
    "\n",
    "    xs: Series\n",
    "    options: options passed to scatter\n",
    "    \"\"\"\n",
    "    options = underride(options, color=\"blue\", alpha=0.2, s=30, edgecolors=\"none\")\n",
    "    ys = xs.values\n",
    "    xs = xs.index\n",
    "    plt.scatter(xs, ys, **options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bdc6e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rolling_mean(daily, label):\n",
    "    \"\"\"Plots rolling mean.\n",
    "\n",
    "    daily: DataFrame of daily prices\n",
    "    \"\"\"\n",
    "    dates = pd.date_range(daily.index.min(), daily.index.max())\n",
    "    reindexed = daily.reindex(dates)\n",
    "    scatter_series(reindexed.ppg, s=15, alpha=0.2, label=label)\n",
    "    roll_mean = pd.Series(reindexed.ppg).rolling(30).mean()\n",
    "    plt.plot(roll_mean, label=\"rolling mean\", color=\"#ff7f00\")\n",
    "    plt.xticks(rotation=30)\n",
    "    decorate(ylabel=\"price per gram ($)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b73a949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rolling_mean(daily, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc7d9fd",
   "metadata": {},
   "source": [
    "The window size is 30, so each value in `roll_mean` is the mean of 30 values from `reindexed.ppg`.\n",
    "The rolling mean seems to do a good job of smoothing out the noise and extracting the trend.\n",
    "The first 29 values are `nan`, and wherever there's a missing value, it's followed by another 29 `nan`s.\n",
    "There are ways to fill in these gaps, but they are a minor nuisance.\n",
    "\n",
    "An alternative is the **exponentially-weighted moving average** (EWMA), which has two advantages.\n",
    "First, as the name suggests, it computes a weighted average where the most recent value has the highest weight and the weights for previous values drop off exponentially.\n",
    "Second, the Pandas implementation of EWMA handles missing values better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "921c5750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ewma(daily, label):\n",
    "    \"\"\"Plots rolling mean.\n",
    "\n",
    "    daily: DataFrame of daily prices\n",
    "    \"\"\"\n",
    "    dates = pd.date_range(daily.index.min(), daily.index.max())\n",
    "    reindexed = daily.reindex(dates)\n",
    "    scatter_series(reindexed.ppg, s=15, alpha=0.2, label=label)\n",
    "    roll_mean = reindexed.ppg.ewm(30).mean()\n",
    "    plt.plot(roll_mean, label=\"EWMA\", color=\"#ff7f00\")\n",
    "    plt.xticks(rotation=30)\n",
    "    decorate(ylabel=\"price per gram ($)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dbd4f4",
   "metadata": {},
   "source": [
    "The **span** parameter corresponds roughly to the window size of a moving average; it controls how fast the weights drop off, so it determines the number of points that make a non-negligible contribution to each average.\n",
    "\n",
    "The following figure shows the EWMA for the same data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5db0f497",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ewma(daily, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8799197c",
   "metadata": {},
   "source": [
    "It is similar to the rolling mean, where they are both defined, but it has no missing values, which makes it easier to work with.\n",
    "The values are noisy at the beginning of the time series, because they are based on fewer data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2c4a00",
   "metadata": {},
   "source": [
    "## Missing values\n",
    "\n",
    "Now that we have characterized the trend of the time series, the next step is to investigate seasonality, which is periodic behavior.\n",
    "Time series data based on human behavior often exhibits daily, weekly, monthly, or yearly cycles.\n",
    "In the next section I present methods to test for seasonality, but they don't work well with missing data, so we have to solve that problem first.\n",
    "\n",
    "A simple and common way to fill missing data is to use a moving average.\n",
    "The `Series` method `fillna` does just what we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b971890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thinkstats import resample\n",
    "\n",
    "\n",
    "def fill_missing(daily, span=30):\n",
    "    \"\"\"Fills missing values with an exponentially weighted moving average.\n",
    "\n",
    "    Resulting DataFrame has new columns 'ewma' and 'resid'.\n",
    "\n",
    "    daily: DataFrame of daily prices\n",
    "    span: window size (sort of) passed to ewma\n",
    "\n",
    "    returns: new DataFrame of daily prices\n",
    "    \"\"\"\n",
    "    dates = pd.date_range(daily.index.min(), daily.index.max())\n",
    "    reindexed = daily.reindex(dates)\n",
    "    ewma = pd.Series(reindexed.ppg).ewm(span=span).mean()\n",
    "    resid = (reindexed.ppg - ewma).dropna()\n",
    "    fake_data = ewma + resample(resid, len(reindexed))\n",
    "    reindexed[\"ppg\"] = reindexed.ppg.fillna(fake_data)\n",
    "    reindexed[\"ewma\"] = ewma\n",
    "    reindexed[\"resid\"] = reindexed.ppg - ewma\n",
    "    return reindexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91422ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filled(daily, label):\n",
    "    \"\"\"Plots the EWMA and filled data.\n",
    "\n",
    "    daily: DataFrame of daily prices\n",
    "    \"\"\"\n",
    "    filled = fill_missing(daily, span=30)\n",
    "    scatter_series(filled.ppg, s=15, alpha=0.2, label=label)\n",
    "    plt.plot(filled.ewma, label=\"EWMA\", color=\"#ff7f00\")\n",
    "    plt.xticks(rotation=30)\n",
    "    decorate(ylabel=\"Price per gram ($)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d78d4f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filled(daily, name)\n",
    "\n",
    "# TODO, what's up with that blip near the end?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041a09b5",
   "metadata": {},
   "source": [
    "Wherever `reindexed.ppg` is `nan`, `fillna` replaces it with the corresponding value from `ewma`.\n",
    "\n",
    "A drawback of this method is that it understates the noise in the series.\n",
    "We can solve that problem by adding in resampled residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "289b4440",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range(daily.index.min(), daily.index.max())\n",
    "reindexed = daily.reindex(dates)\n",
    "ewma = pd.Series(reindexed.ppg).ewm(span=30).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4799503",
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = (reindexed.ppg - ewma).dropna()\n",
    "fake_data = ewma + resample(resid, len(reindexed))\n",
    "reindexed[\"ppg\"] = reindexed.ppg.fillna(ewma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41681af7",
   "metadata": {},
   "source": [
    "`resid` contains the residual values, not including days when `ppg` is `nan`.\n",
    "`fake_data` contains the sum of the moving average and a random sample of residuals.\n",
    "Finally, `fillna` replaces `nan` with values from `fake_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "203972f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: missing figure in the sequence here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3640adb",
   "metadata": {},
   "source": [
    "The filled data is visually similar to the actual values.\n",
    "Since the resampled residuals are random, the results are different every time; later we'll see how to characterize the error created by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f85f8f4",
   "metadata": {},
   "source": [
    "## Serial correlation\n",
    "\n",
    "As prices vary from day to day, you might expect to see patterns.\n",
    "If the price is high on Monday, you might expect it to be high for a few more days; and if it's low, you might expect it to stay low.\n",
    "A pattern like this is called **serial correlation**, because each value is correlated with the next one in the series.\n",
    "\n",
    "To compute serial correlation, we can shift the time series by an interval called a **lag**, and then compute the correlation of the shifted series with the original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7889d674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thinkstats import corr\n",
    "\n",
    "\n",
    "def serial_corr(series, lag=1):\n",
    "    xs = series[lag:]\n",
    "    ys = series.shift(lag)[lag:]\n",
    "    return corr(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50af8211",
   "metadata": {},
   "source": [
    "After the shift, the first `lag` values are `nan`, so I use a slice to remove them before computing `Corr`.\n",
    "\n",
    "If we apply `SerialCorr` to the raw price data with lag 1, we find serial correlation 0.48 for the high quality category, 0.16 for medium and 0.10 for low.\n",
    "In any time series with a long-term trend, we expect to see strong serial correlations; for example, if prices are falling, we expect to see values above the mean in the first half of the series and values below the mean in the second half.\n",
    "\n",
    "It is more interesting to see if the correlation persists if you subtract away the trend.\n",
    "For example, we can compute the residual of the EWMA and then compute its serial correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ccfb707",
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_dailies = {}\n",
    "for name, daily in dailies.items():\n",
    "    filled_dailies[name] = fill_missing(daily, span=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "986c9499",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, filled in filled_dailies.items():\n",
    "    r = serial_corr(filled.ppg, lag=1)\n",
    "    print(name, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb36a4c",
   "metadata": {},
   "source": [
    "With lag=1, the serial correlations for the de-trended data are -0.022 for high quality, -0.015 for medium, and 0.036 for low.\n",
    "These values are small, indicating that there is little or no one-day serial correlation in this series.\n",
    "\n",
    "To check for weekly, monthly, and yearly seasonality, I ran the analysis again with different lags.\n",
    "Here are the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15c7d0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, filled in filled_dailies.items():\n",
    "    r = serial_corr(filled.resid, lag=1)\n",
    "    print(name, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d0da0e",
   "metadata": {},
   "source": [
    "In the next section we'll test whether these correlations are statistically significant (they are not), but at this point we can tentatively conclude that there are no substantial seasonal patterns in these series, at least not with these lags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa34bd1",
   "metadata": {},
   "source": [
    "## Autocorrelation\n",
    "\n",
    "If you think a series might have some serial correlation, but you don't know which lags to test, you can test them all! The **autocorrelation function** is a function that maps from lag to the serial correlation with the given lag.\n",
    "\"Autocorrelation\" is another name for serial correlation, used more often when the lag is not 1.\n",
    "\n",
    "StatsModels provides functions for time series analysis, including `acf`, which computes the autocorrelation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "158e24e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.tsa.stattools as smtsa\n",
    "\n",
    "filled = filled_dailies[\"high\"]\n",
    "acf = smtsa.acf(filled.resid, nlags=365, adjusted=True, fft=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefa026f",
   "metadata": {},
   "source": [
    "`acf` computes serial correlations with lags from 0 through `nlags`.\n",
    "The `unbiased` flag tells `acf` to correct the estimates for the sample size.\n",
    "The result is an array of correlations.\n",
    "If we select daily prices for high quality, and extract correlations for lags 1, 7, 30, and 365, we can confirm that `acf` and `SerialCorr` yield approximately the same results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca4b8f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{acf[0]:.2} {acf[1]:.2} {acf[7]:.2} {acf[30]:.2} {acf[365]:.2}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106ad044",
   "metadata": {},
   "source": [
    "With `lag=0`, `acf` computes the correlation of the series with itself, which is always 1.\n",
    "\n",
    "\n",
    "The following figure shows autocorrelation functions for the three quality categories, with `nlags=40`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae946e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_autocorrelation(daily, iters=1001, nlags=40):\n",
    "    \"\"\"Resample residuals, compute autocorrelation, and plot percentiles.\n",
    "\n",
    "    daily: DataFrame\n",
    "    iters: number of simulations to run\n",
    "    nlags: maximum lags to compute autocorrelation\n",
    "    \"\"\"\n",
    "    t = []\n",
    "    for _ in range(iters):\n",
    "        filled = fill_missing(daily, span=30)\n",
    "        resid = resample(filled.resid)\n",
    "        acf = smtsa.acf(resid, nlags=nlags, adjusted=True, fft=False)[1:]\n",
    "        t.append(np.abs(acf))\n",
    "    high = percentile_rows(t, [97.5])[0]\n",
    "    low = -high\n",
    "    lags = range(1, nlags + 1)\n",
    "    plt.fill_between(lags, low, high, alpha=0.2, color=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b6a7ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thinkstats import percentile_rows\n",
    "\n",
    "\n",
    "def plot_auto_correlation(dailies, nlags=40, add_weekly=False):\n",
    "    \"\"\"Plots autocorrelation functions.\n",
    "\n",
    "    dailies: map from category name to DataFrame of daily prices\n",
    "    nlags: number of lags to compute\n",
    "    add_weekly: boolean, whether to add a simulated weekly pattern\n",
    "    \"\"\"\n",
    "    daily = dailies[\"high\"]\n",
    "    simulate_autocorrelation(daily)\n",
    "    for name, daily in dailies.items():\n",
    "        if add_weekly:\n",
    "            daily.ppg = add_weekly_seasonality(daily)\n",
    "        filled = fill_missing(daily, span=30)\n",
    "        acf = smtsa.acf(filled.resid, nlags=nlags, adjusted=True, fft=False)\n",
    "        lags = np.arange(len(acf))\n",
    "        plt.plot(lags[1:], acf[1:], label=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1203c5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "axis = [0, 41, -0.2, 0.2]\n",
    "plot_auto_correlation(dailies, add_weekly=False)\n",
    "decorate(xlabel=\"lag (day)\", ylabel=\"correlation\", loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386efad9",
   "metadata": {},
   "source": [
    "The gray region shows the normal variability we would expect if there is no actual autocorrelation; anything that falls outside this range is statistically significant, with a p-value less than 5%. Since the false positive rate is 5%, and we are computing 120 correlations (40 lags for each of 3 times series), we expect to see about 6 points outside this region.\n",
    "In fact, there are 7. We conclude that there are no autocorrelations in these series that could not be explained by chance.\n",
    "\n",
    "I computed the gray regions by resampling the residuals.\n",
    "You can see my code in `timeseries.py`; the function is called `SimulateAutocorrelation`.\n",
    "\n",
    "To see what the autocorrelation function looks like when there is a seasonal component, I generated simulated data by adding a weekly cycle.\n",
    "Assuming that demand for cannabis is higher on weekends, we might expect the price to be higher.\n",
    "To simulate this effect, I select dates that fall on Friday or Saturday and add a random amount to the price, chosen from a uniform distribution from \\$0 to \\$2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa4c5819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weekly_seasonality(daily):\n",
    "    \"\"\"Adds a weekly pattern.\n",
    "\n",
    "    daily: DataFrame of daily prices\n",
    "\n",
    "    returns: new DataFrame of daily prices\n",
    "    \"\"\"\n",
    "    fri_or_sat = (daily.index.dayofweek == 4) | (daily.index.dayofweek == 5)\n",
    "    fake = daily.ppg.copy()\n",
    "    fake[fri_or_sat] += np.random.uniform(0, 2, fri_or_sat.sum())\n",
    "    return fake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402d192f",
   "metadata": {},
   "source": [
    "`frisat` is a boolean `Series`, `True` if the day of the week is Friday or Saturday.\n",
    "`fake` is a new `DataFrame`, initially a copy of `daily`, which we modify by adding random values to `ppg`.\n",
    "`frisat.sum()` is the total number of Fridays and Saturdays, which is the number of random values we have to generate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7632f5",
   "metadata": {},
   "source": [
    "The following figure shows autocorrelation functions for prices with this simulated seasonality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b7d695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "axis = [0, 41, -0.2, 0.2]\n",
    "plot_auto_correlation(dailies, add_weekly=True)\n",
    "decorate(xlabel=\"lag (day)\", ylabel=\"correlation\", loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6683f1e",
   "metadata": {},
   "source": [
    "As expected, the correlations are highest when the lag is a multiple of 7. For high and medium quality, the new correlations are statistically significant.\n",
    "For low quality they are not, because residuals in this category are large; the effect would have to be bigger to be visible through the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4efa619",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Time series analysis can be used to investigate, and sometimes explain, the behavior of systems that vary in time.\n",
    "It can also make predictions.\n",
    "\n",
    "The linear regressions we used in Section [\\[timeregress\\]](#timeregress){reference-type=\"ref\" reference=\"timeregress\"} can be used for prediction.\n",
    "The RegressionResults class provides `predict`, which takes a `DataFrame` containing the explanatory variables and returns a sequence of predictions.\n",
    "Here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "896be10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simple_prediction(results, years):\n",
    "    \"\"\"Generates a simple prediction.\n",
    "\n",
    "    results: results object\n",
    "    years: sequence of times (in years) to make predictions for\n",
    "\n",
    "    returns: sequence of predicted values\n",
    "    \"\"\"\n",
    "    n = len(years)\n",
    "    inter = np.ones(n)\n",
    "    d = dict(Intercept=inter, years=years, years2=years**2)\n",
    "    predict_df = pd.DataFrame(d)\n",
    "    predict = results.predict(predict_df)\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aca1746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simple_prediction(name, results, years):\n",
    "    predict = generate_simple_prediction(results, years)\n",
    "    plt.scatter(daily.years, daily.ppg, alpha=0.2, label=name)\n",
    "    plt.plot(years, predict, color=\"#ff7f00\")\n",
    "    xlim = years[0] - 0.1, years[-1] + 0.1\n",
    "    decorate(\n",
    "        title=\"Predictions\",\n",
    "        xlabel=\"Years\",\n",
    "        xlim=xlim,\n",
    "        ylabel=\"Price per gram ($)\",\n",
    "        loc=\"upper right\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f0c2f9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"high\"\n",
    "daily = dailies[name]\n",
    "_, results = run_linear_model(daily)\n",
    "years = np.linspace(0, 5, 101)\n",
    "plot_simple_prediction(name, results, years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03969cdb",
   "metadata": {},
   "source": [
    "`results` is a RegressionResults object; `years` is the sequence of time values we want predictions for.\n",
    "The function constructs a `DataFrame`, passes it to `predict`, and returns the result.\n",
    "\n",
    "If all we want is a single, best-guess prediction, we're done.\n",
    "But for most purposes it is important to quantify error.\n",
    "In other words, we want to know how accurate the prediction is likely to be.\n",
    "\n",
    "There are three sources of error we should take into account:\n",
    "\n",
    "-   Sampling error: The prediction is based on estimated parameters, which depend on random variation in the sample.\n",
    "If we run the experiment again, we expect the estimates to vary.\n",
    "\n",
    "-   Random variation: Even if the estimated parameters are perfect, the observed data varies randomly around the long-term trend, and we expect this variation to continue in the future.\n",
    "\n",
    "-   Modeling error: We have already seen evidence that the long-term trend is not linear, so predictions based on a linear model will eventually fail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a817f214",
   "metadata": {},
   "source": [
    "Another source of error to consider is unexpected future events.\n",
    "Agricultural prices are affected by weather, and all prices are affected by politics and law.\n",
    "As I write this, cannabis is legal in two states and legal for medical purposes in 20 more.\n",
    "If more states legalize it, the price is likely to go down.\n",
    "But if the federal government cracks down, the price might go up.\n",
    "\n",
    "Modeling errors and unexpected future events are hard to quantify.\n",
    "Sampling error and random variation are easier to deal with, so we'll do that first.\n",
    "\n",
    "To quantify sampling error, I use resampling, as we did in Section [\\[regest\\]](#regest){reference-type=\"ref\" reference=\"regest\"}.\n",
    "As always, the goal is to use the actual observations to simulate what would happen if we ran the experiment again.\n",
    "The simulations are based on the assumption that the estimated parameters are correct, but the random residuals could have been different.\n",
    "Here is a function that runs the simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f48583f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_results(daily, iters=101, func=run_linear_model):\n",
    "    \"\"\"Run simulations based on resampling residuals.\n",
    "\n",
    "    daily: DataFrame of daily prices\n",
    "    iters: number of simulations\n",
    "    func: function that fits a model to the data\n",
    "\n",
    "    returns: list of result objects\n",
    "    \"\"\"\n",
    "    _, results = func(daily)\n",
    "    fake = daily.copy()\n",
    "    result_seq = []\n",
    "    for _ in range(iters):\n",
    "        fake.ppg = results.fittedvalues + resample(results.resid)\n",
    "        _, fake_results = func(fake)\n",
    "        result_seq.append(fake_results)\n",
    "    return result_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c729adbf",
   "metadata": {},
   "source": [
    "`daily` is a `DataFrame` containing the observed prices; `iters` is the number of simulations to run.\n",
    "\n",
    "`SimulateResults` uses `RunLinearModel`, from Section [\\[timeregress\\]](#timeregress){reference-type=\"ref\" reference=\"timeregress\"}, to estimate the slope and intercept of the observed values.\n",
    "\n",
    "Each time through the loop, it generates a \"fake\" dataset by resampling the residuals and adding them to the fitted values.\n",
    "Then it runs a linear model on the fake data and stores the RegressionResults object.\n",
    "\n",
    "The next step is to use the simulated results to generate predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8768e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(result_seq, years, add_resid=False):\n",
    "    \"\"\"Generates an array of predicted values from a list of model results.\n",
    "\n",
    "    When add_resid is False, predictions represent sampling error only.\n",
    "\n",
    "    When add_resid is True, they also include residual error (which is\n",
    "    more relevant to prediction).\n",
    "\n",
    "    result_seq: list of model results\n",
    "    years: sequence of times (in years) to make predictions for\n",
    "    add_resid: boolean, whether to add in resampled residuals\n",
    "\n",
    "    returns: sequence of predictions\n",
    "    \"\"\"\n",
    "    n = len(years)\n",
    "    d = dict(Intercept=np.ones(n), years=years, years2=years**2)\n",
    "    predict_df = pd.DataFrame(d)\n",
    "    predict_seq = []\n",
    "    for fake_results in result_seq:\n",
    "        predict = fake_results.predict(predict_df)\n",
    "        if add_resid:\n",
    "            predict += resample(fake_results.resid, n)\n",
    "        predict_seq.append(predict)\n",
    "    return predict_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c62df54",
   "metadata": {},
   "source": [
    "`GeneratePredictions` takes the sequence of results from the previous step, as well as `years`, which is a sequence of floats that specifies the interval to generate predictions for, and `add_resid`, which indicates whether it should add resampled residuals to the straight-line prediction.\n",
    "`GeneratePredictions` iterates through the sequence of RegressionResults and generates a sequence of predictions.\n",
    "\n",
    "\n",
    "Finally, here's the code that plots a 90% confidence interval for the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23a3457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(daily, years, iters=101, percent=90, func=run_linear_model):\n",
    "    \"\"\"Plots predictions.\n",
    "\n",
    "    daily: DataFrame of daily prices\n",
    "    years: sequence of times (in years) to make predictions for\n",
    "    iters: number of simulations\n",
    "    percent: what percentile range to show\n",
    "    func: function that fits a model to the data\n",
    "    \"\"\"\n",
    "    result_seq = simulate_results(daily, iters=iters, func=func)\n",
    "    p = (100 - percent) / 2\n",
    "    percents = p, 100 - p\n",
    "    predict_seq = generate_predictions(result_seq, years, add_resid=True)\n",
    "    low, high = percentile_rows(predict_seq, percents)\n",
    "    plt.fill_between(years, low, high, alpha=0.3, color=\"gray\")\n",
    "\n",
    "    predict_seq = generate_predictions(result_seq, years, add_resid=False)\n",
    "    low, high = percentile_rows(predict_seq, percents)\n",
    "    plt.fill_between(years, low, high, alpha=0.5, color=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a876a5",
   "metadata": {},
   "source": [
    "`PlotPredictions` calls `GeneratePredictions` twice: once with `add_resid=True` and again with `add_resid=False`.\n",
    "It uses `PercentileRows` to select the 5th and 95th percentiles for each year, then plots a gray region between these bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727e6d16",
   "metadata": {},
   "source": [
    "The following figure shows the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "30cd624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = np.linspace(0, 5, 101)\n",
    "plt.scatter(daily.years, daily.ppg, alpha=0.1, label=name)\n",
    "plot_predictions(daily, years)\n",
    "xlim = years[0] - 0.1, years[-1] + 0.1\n",
    "decorate(title=\"Predictions\", xlabel=\"Years\", xlim=xlim, ylabel=\"Price per gram ($)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e57dfa",
   "metadata": {},
   "source": [
    "The dark gray region represents a 90% confidence interval for the sampling error; that is, uncertainty about the estimated slope and intercept due to sampling.\n",
    "\n",
    "The lighter region shows a 90% confidence interval for prediction error, which is the sum of sampling error and random variation.\n",
    "\n",
    "These regions quantify sampling error and random variation, but not modeling error.\n",
    "In general modeling error is hard to quantify, but in this case we can address at least one source of error, unpredictable external events.\n",
    "\n",
    "The regression model is based on the assumption that the system is **stationary**; that is, that the parameters of the model don't change over time.\n",
    "Specifically, it assumes that the slope and intercept are constant, as well as the distribution of residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfa6771",
   "metadata": {},
   "source": [
    "But looking at the moving averages in [which figure?], it seems like the slope changes at least once during the observed interval, and the variance of the residuals seems bigger in the first half than the second.\n",
    "\n",
    "As a result, the parameters we get depend on the interval we observe.\n",
    "To see how much effect this has on the predictions, we can extend `simulate_results` to use intervals of observation with different start and end dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "70ce12aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_intervals(daily, iters=101, func=run_linear_model):\n",
    "    \"\"\"Run simulations based on different subsets of the data.\n",
    "\n",
    "    daily: DataFrame of daily prices\n",
    "    iters: number of simulations\n",
    "    func: function that fits a model to the data\n",
    "\n",
    "    returns: list of result objects\n",
    "    \"\"\"\n",
    "    result_seq = []\n",
    "    starts = np.linspace(0, len(daily), iters).astype(int)\n",
    "    for start in starts[:-2]:\n",
    "        subset = daily[start:]\n",
    "        _, results = func(subset)\n",
    "        fake = subset.copy()\n",
    "        for _ in range(iters):\n",
    "            fake.ppg = results.fittedvalues + resample(results.resid)\n",
    "            _, fake_results = func(fake)\n",
    "            result_seq.append(fake_results)\n",
    "    return result_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "becc488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_intervals(daily, years, iters=101, percent=90, func=run_linear_model):\n",
    "    \"\"\"Plots predictions based on different intervals.\n",
    "\n",
    "    daily: DataFrame of daily prices\n",
    "    years: sequence of times (in years) to make predictions for\n",
    "    iters: number of simulations\n",
    "    percent: what percentile range to show\n",
    "    func: function that fits a model to the data\n",
    "    \"\"\"\n",
    "    result_seq = simulate_intervals(daily, iters=iters, func=func)\n",
    "    p = (100 - percent) / 2\n",
    "    percents = p, 100 - p\n",
    "    predict_seq = generate_predictions(result_seq, years, add_resid=True)\n",
    "    low, high = percentile_rows(predict_seq, percents)\n",
    "    plt.fill_between(years, low, high, alpha=0.2, color=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12596f37",
   "metadata": {},
   "source": [
    "The following figure shows the result for the medium quality category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "39bb8432",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"high\"\n",
    "daily = dailies[name]\n",
    "plt.scatter(daily.years, daily.ppg, alpha=0.1, label=name)\n",
    "\n",
    "plot_intervals(daily, years)\n",
    "plot_predictions(daily, years)\n",
    "xlim = years[0] - 0.1, years[-1] + 0.1\n",
    "decorate(title=\"Predictions\", xlabel=\"Years\", xlim=xlim, ylabel=\"Price per gram ($)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a0b2d2",
   "metadata": {},
   "source": [
    "The lightest gray area shows a confidence interval that includes uncertainty due to sampling error, random variation, and variation in the interval of observation.\n",
    "\n",
    "The model based on the entire interval has positive slope, indicating that prices were increasing.\n",
    "But the most recent interval shows signs of decreasing prices, so models based on the most recent data have negative slope.\n",
    "As a result, the widest predictive interval includes the possibility of decreasing prices over the next year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179eb683",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "Time series analysis is a big topic; this chapter has only scratched the surface.\n",
    "An important tool for working with time series data is autoregression, which I did not cover here, mostly because it turns out not to be useful for the example data I worked with.\n",
    "\n",
    "But once you have learned the material in this chapter, you are well prepared to learn about autoregression.\n",
    "One resource I recommend is Philipp Janert's book, *Data Analysis with Open Source Tools*, O'Reilly Media, 2011.\n",
    "His chapter on time series analysis picks up where this one leaves off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4799b6",
   "metadata": {},
   "source": [
    "## Glossary\n",
    "\n",
    "-   **time series**: A dataset where each value is associated with a timestamp, often a series of measurements and the times they were collected.\n",
    "\n",
    "-   **window**: A sequence of consecutive values in a time series, often used to compute a moving average.\n",
    "\n",
    "-   **moving average**: One of several statistics intended to estimate the underlying trend in a time series by computing averages (of some kind) for a series of overlapping windows.\n",
    "\n",
    "-   **rolling mean**: A moving average based on the mean value in each window.\n",
    "\n",
    "-   **exponentially-weighted moving average (EWMA)**: A moving average based on a weighted mean that gives the highest weight to the most recent values, and exponentially decreasing weights to earlier values.\n",
    "\n",
    "-   **span**: A parameter of EWMA that determines how quickly the weights decrease.\n",
    "\n",
    "-   **serial correlation**: Correlation between a time series and a shifted or lagged version of itself.\n",
    "\n",
    "-   **lag**: The size of the shift in a serial correlation or autocorrelation.\n",
    "\n",
    "-   **autocorrelation**: A more general term for a serial correlation with any amount of lag.\n",
    "\n",
    "-   **autocorrelation function**: A function that maps from lag to serial correlation.\n",
    "\n",
    "-   **stationary**: A model is stationary if the parameters and the distribution of residuals does not change over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce1ad7f",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b8a9a9",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Exercise:**   The linear model I used in this chapter has the obvious drawback that it is linear, and there is no reason to expect prices to change linearly over time.\n",
    "We can add flexibility to the model by adding a quadratic term, as we did in Section 11.3.\n",
    "\n",
    "Use a quadratic model to fit the time series of daily prices, and use the model to generate predictions.\n",
    "You will have to write a version of `RunLinearModel` that runs that quadratic model, but after that you should be able to reuse code from the chapter to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8bd5d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_quadratic_model(daily):\n",
    "    \"\"\"Runs a linear model of prices versus years.\n",
    "\n",
    "    daily: DataFrame of daily prices\n",
    "\n",
    "    returns: model, results\n",
    "    \"\"\"\n",
    "    daily[\"years2\"] = daily.years**2\n",
    "    model = smf.ols(\"ppg ~ years + years2\", data=daily)\n",
    "    results = model.fit()\n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "16ea9ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"high\"\n",
    "daily = dailies[name]\n",
    "model, results = run_quadratic_model(daily)\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fd17e315",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fitted_values(model, results, label=name)\n",
    "decorate(\n",
    "    title=\"Fitted values\", xlabel=\"Years\", xlim=[-0.1, 3.8], ylabel=\"price per gram ($)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "db3571f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = np.linspace(0, 5, 101)\n",
    "plt.scatter(daily.years, daily.ppg, alpha=0.1, label=name)\n",
    "plot_predictions(daily, years, func=run_quadratic_model)\n",
    "decorate(\n",
    "    title=\"predictions\",\n",
    "    xlabel=\"Years\",\n",
    "    xlim=[years[0] - 0.1, years[-1] + 0.1],\n",
    "    ylabel=\"Price per gram ($)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41616638",
   "metadata": {},
   "source": [
    "**Exercise:** Write a definition for a class named `SerialCorrelationTest` that extends `HypothesisTest` from Section 9.2. It should take a series and a lag as data, compute the serial correlation of the series with the given lag, and then compute the p-value of the observed correlation.\n",
    "\n",
    "Use this class to test whether the serial correlation in raw price data is statistically significant.\n",
    "Also test the residuals of the linear model and (if you did the previous exercise), the quadratic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c501ee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thinkstats import HypothesisTest\n",
    "\n",
    "\n",
    "class SerialCorrelationTest(HypothesisTest):\n",
    "    \"\"\"Tests serial correlations by permutation.\"\"\"\n",
    "\n",
    "    def test_statistic(self, data):\n",
    "        \"\"\"Computes the test statistic.\n",
    "\n",
    "        data: tuple of xs and ys\n",
    "        \"\"\"\n",
    "        series, lag = data\n",
    "        test_stat = abs(serial_corr(series, lag))\n",
    "        return test_stat\n",
    "\n",
    "    def run_model(self):\n",
    "        \"\"\"Run the model of the null hypothesis.\n",
    "\n",
    "        returns: simulated data\n",
    "        \"\"\"\n",
    "        series, lag = self.data\n",
    "        permutation = series.reindex(np.random.permutation(series.index))\n",
    "        return permutation, lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3d40ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"high\"\n",
    "daily = dailies[name]\n",
    "series = daily.ppg\n",
    "test = SerialCorrelationTest((series, 1))\n",
    "pvalue = test.p_value()\n",
    "print(test.actual, pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c6598d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, results = run_linear_model(daily)\n",
    "series = results.resid\n",
    "test = SerialCorrelationTest((series, 1))\n",
    "pvalue = test.p_value()\n",
    "print(test.actual, pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "65eb37f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, results = run_quadratic_model(daily)\n",
    "series = results.resid\n",
    "test = SerialCorrelationTest((series, 1))\n",
    "pvalue = test.p_value()\n",
    "print(test.actual, pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41598c42",
   "metadata": {},
   "source": [
    "**Bonus Example:** There are several ways to extend the EWMA model to generate predictions.\n",
    "One of the simplest is something like this:\n",
    "\n",
    "1. Compute the EWMA of the time series and use the last point as an intercept, `inter`.\n",
    "\n",
    "2. Compute the EWMA of differences between successive elements in the time series and use the last point as a slope, `slope`.\n",
    "\n",
    "3. To predict values at future times, compute `inter + slope * dt`, where `dt` is the difference between the time of the prediction and the time of the last observation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "20b538fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"high\"\n",
    "daily = dailies[name]\n",
    "filled = fill_missing(daily)\n",
    "diffs = filled.ppg.diff()\n",
    "plt.plot(diffs)\n",
    "plt.xticks(rotation=30)\n",
    "decorate(ylabel=\"Daily change in price per gram ($)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b733ae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filled[\"slope\"] = diffs.ewm(span=365).mean()\n",
    "plt.plot(filled.slope[-365:])\n",
    "plt.xticks(rotation=30)\n",
    "decorate(ylabel=\"EWMA of diff ($)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d6623def",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = filled.index[-1]\n",
    "inter = filled.ewma.iloc[-1]\n",
    "slope = filled.slope[-30:].mean()\n",
    "start, inter, slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f8e9298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range(filled.index.min(), filled.index.max() + np.timedelta64(365, \"D\"))\n",
    "predicted = filled.reindex(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "95bb549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted[\"date\"] = predicted.index\n",
    "one_day = np.timedelta64(1, \"D\")\n",
    "predicted[\"days\"] = (predicted.date - start) / one_day\n",
    "predict = inter + slope * predicted.days\n",
    "predicted[\"ewma\"] = predicted.ewma.fillna(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "360180d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_series(daily.ppg, alpha=0.1, label=name)\n",
    "plt.plot(predicted.ewma, color=\"#ff7f00\")\n",
    "decorate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a6ab5f",
   "metadata": {},
   "source": [
    "As an exercise, run this analysis again for the other quality categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7af613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
