{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52c53da4",
   "metadata": {},
   "source": [
    "# Analytic Methods\n",
    "\n",
    "This book has focused on computational methods like simulation and resampling, but some of the problems we solved have analytic solutions that can be much faster to compute.\n",
    "\n",
    "This chapter presents some of these methods and explains how they work.\n",
    "At the end of the chapter, I make suggestions for integrating computational and analytic methods for data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a92b7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "[Click here to run this notebook on Colab](https://colab.research.google.com/github/AllenDowney/ThinkStats/blob/v3/nb/chap14.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e0ae6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f61641d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os.path import basename, exists\n",
    "\n",
    "\n",
    "def download(url):\n",
    "    filename = basename(url)\n",
    "    if not exists(filename):\n",
    "        from urllib.request import urlretrieve\n",
    "\n",
    "        local, _ = urlretrieve(url, filename)\n",
    "        print(\"Downloaded \" + local)\n",
    "\n",
    "\n",
    "download(\"https://github.com/AllenDowney/ThinkStats/raw/v3/nb/thinkstats.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfb3ff15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import empiricaldist\n",
    "except ImportError:\n",
    "    !pip install empiricaldist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "101a23bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from thinkstats import decorate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66b3b13",
   "metadata": {},
   "source": [
    "## Normal Probability Plots\n",
    "\n",
    "Many analytic methods are based on the properties of the normal distribution, for two reasons: distributions of many measurements in the real world are well-approximated by normal distributions, and normal distributions have mathematical properties that make them useful for analysis.\n",
    "\n",
    "To demonstrate the first point, we'll look at some of the measurements in the penguin dataset, which we saw in Section XXX.\n",
    "Then we'll explore the mathematical properties of the normal distribution.\n",
    "Instructions for downloading the penguin dataset are in the notebook for this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3e7431",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following cell downloads the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5305102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "download(\n",
    "    \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/c19a904462482430170bfe2c718775ddb7dbb885/inst/extdata/penguins_raw.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ecea73",
   "metadata": {},
   "source": [
    "We can read the data like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b616c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = pd.read_csv(\"penguins_raw.csv\")\n",
    "penguins.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf96d0ee",
   "metadata": {},
   "source": [
    "The dataset contains measurements from three penguin species.\n",
    "For this example, we'll select the Adélie penguins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90473d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "adelie = penguins.query('Species.str.startswith(\"Adelie\")').copy()\n",
    "len(adelie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc1881b",
   "metadata": {},
   "source": [
    "To see if penguin weights follow a normal distribution, we'll compute the empirical CDF of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "125f106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from empiricaldist import Cdf\n",
    "\n",
    "weights = adelie[\"Body Mass (g)\"].dropna()\n",
    "cdf_weights = Cdf.from_seq(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ad44ce",
   "metadata": {},
   "source": [
    "And we'll compute the analytic CDF of a normal distribution with the same mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82688ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, s = weights.mean(), weights.std()\n",
    "m, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a076e36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "dist = norm(m, s)\n",
    "qs = np.linspace(m - 3.5 * s, m + 3.5 * s)\n",
    "ps = dist.cdf(qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ddb849",
   "metadata": {},
   "source": [
    "Here's what the CDF of the data looks like compared to the normal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b992d5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(qs, ps, color=\"gray\", alpha=0.5, label=\"Normal model\")\n",
    "cdf_weights.plot(label=\"data\")\n",
    "\n",
    "decorate(ylabel=\"CDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7780bb",
   "metadata": {},
   "source": [
    "The normal distribution might be a good enough model of this data, but it's certainly not a perfect fit.\n",
    "\n",
    "In general, plotting the CDF of the data and the CDF of a model is a good way to evaluate how well the model fits the data.\n",
    "But one drawback of this method is that it depends on how well we estimate the parameters of the model -- in this example, the mean and standard deviation.\n",
    "\n",
    "An alternative is a **normal probability plot**, which does not depend on our ability to estimate parameters.\n",
    "\n",
    "\n",
    "In a normal probability plot the $y$ values are the sorted measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0638de99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = np.sort(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc490c0",
   "metadata": {},
   "source": [
    "And the $x$ values are the corresponding percentiles of a normal distribution, computed using the `ppf` method of the `norm` object, which computes the inverse CDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d24b276",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(weights)\n",
    "xs = norm.ppf((np.arange(n) + 0.5) / n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9860c3fb",
   "metadata": {},
   "source": [
    "If the measurements are actually drawn from a normal distribution, the $y$ and $x$ values should fall on a straight line.\n",
    "To see how well they do, we can use `linregress` to fit a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1bebe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "\n",
    "results = linregress(xs, ys)\n",
    "intercept, slope = results.intercept, results.slope\n",
    "\n",
    "fit_xs = np.linspace(-3, 3)\n",
    "fit_ys = intercept + slope * fit_xs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020a663a",
   "metadata": {},
   "source": [
    "The following figure shows the $x$ and $y$ values along with the fitted line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdcc30c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fit_xs, fit_ys, color=\"gray\", alpha=0.5, label=\"model\")\n",
    "plt.plot(xs, ys, label=\"data\")\n",
    "\n",
    "decorate(xlabel=\"Standard normal\", ylabel=\"Body mass (g)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5381ed78",
   "metadata": {},
   "source": [
    "The normal probability plot is not a perfectly straight line, which indicates that the normal distribution is not a perfect model for this data.\n",
    "One reason is that the dataset includes male and female penguins, so let's see what happens if we plot the two groups separately.\n",
    "The following function encapsulates the steps we used to make a normal probability plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "976ff517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_probability_plot(sample, **options):\n",
    "    \"\"\"Makes a normal probability plot with a fitted line.\"\"\"\n",
    "    n = len(sample)\n",
    "    xs = norm.ppf((np.arange(n) + 0.5) / n)\n",
    "    ys = np.sort(sample)\n",
    "\n",
    "    results = linregress(xs, ys)\n",
    "    intercept, slope = results.intercept, results.slope\n",
    "\n",
    "    fit_xs = np.linspace(-3, 3)\n",
    "    fit_ys = intercept + slope * fit_xs\n",
    "\n",
    "    plt.plot(fit_xs, fit_ys, color=\"gray\", alpha=0.5)\n",
    "    plt.plot(xs, ys, **options)\n",
    "    decorate(xlabel=\"Standard normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2447c63",
   "metadata": {},
   "source": [
    "Here's what the results look like for male and female penguins separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9ce4632",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = adelie.groupby(\"Sex\")\n",
    "\n",
    "weights_male = grouped.get_group(\"MALE\")[\"Body Mass (g)\"]\n",
    "normal_probability_plot(weights_male, ls=\"--\", label=\"Male\")\n",
    "\n",
    "weights_female = grouped.get_group(\"FEMALE\")[\"Body Mass (g)\"]\n",
    "normal_probability_plot(weights_female, label=\"Female\")\n",
    "\n",
    "decorate(ylabel=\"Weight (g)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb725e29",
   "metadata": {},
   "source": [
    "The normal probability plots for both groups are close to a straight line, which indicates that the distributions of weight follow normal distributions.\n",
    "When we put the groups together, the distribution of their weights is a mixture of two normal distributions with different means -- and a mixture like that is not always well modeled by a normal distribution.\n",
    "\n",
    "Now let's consider some of the mathematical properties of normal distributions that make them so useful for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249a2d83",
   "metadata": {},
   "source": [
    "## Normal Distributions\n",
    "\n",
    "The following class defines an object that represents a normal distribution.\n",
    "It contains as attributes the parameters `mu` and `sigma2`, which represent the mean and variance of the distribution.\n",
    "The name `sigma2` is a reminder that variance is the square of the standard deviation, which is usually denoted `sigma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0baee14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normal:\n",
    "    \"\"\"Represents a Normal distribution\"\"\"\n",
    "\n",
    "    def __init__(self, mu, sigma2):\n",
    "        \"\"\"Make a Normal object.\n",
    "\n",
    "        mu: mean\n",
    "        sigma2: variance\n",
    "        \"\"\"\n",
    "        self.mu = mu\n",
    "        self.sigma2 = sigma2\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Returns a string representation.\"\"\"\n",
    "        return f\"Normal({self.mu}, {self.sigma2})\"\n",
    "\n",
    "    __str__ = __repr__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c0586b",
   "metadata": {},
   "source": [
    "As an example, we'll create a `Normal` object that represents a normal distribution with the same mean and variance as the weights of the male penguins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1efbcfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, s = weights_male.mean(), weights_male.std()\n",
    "dist_male = Normal(m, s**2)\n",
    "dist_male"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1401fdd9",
   "metadata": {},
   "source": [
    "And another `Normal` object with the same mean and variance as the weights of the female penguins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ecc2c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, s = weights_female.mean(), weights_female.std()\n",
    "dist_female = Normal(m, s**2)\n",
    "dist_female"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4025ae97",
   "metadata": {},
   "source": [
    "Next we'll add a method to the `Normal` class that generates a random sample from a normal distribution.\n",
    "To add methods to an existing class, we'll use a Jupyter magic command, `add_method_to`, which is defined in the `thinkstats` module.\n",
    "This command is not part of Python -- it only works in Jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5dacba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_method_to Normal\n",
    "\n",
    "def sample(self, n):\n",
    "    \"\"\"Generate a random sample from this distribution.\"\"\"\n",
    "    sigma = np.sqrt(self.sigma2)\n",
    "    return np.random.normal(self.mu, sigma, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6fa3fe",
   "metadata": {},
   "source": [
    "We'll use `sample` to demonstrate the first useful property of a normal distribution: if you draw values from two normal distributions and add them, the distribution of the sum is also normal.\n",
    "\n",
    "As an example, we'll generate samples from the `Normal` objects we just made, add them together, and make a normal probability plot of the sums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f44bed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sum = dist_male.sample(1000) + dist_female.sample(1000)\n",
    "normal_probability_plot(sample_sum)\n",
    "\n",
    "decorate(ylabel=\"Total weight (g)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec23c17",
   "metadata": {},
   "source": [
    "The normal probability plot looks like a straight line, which indicates that the sums follow a normal distribution.\n",
    "And that's not all -- if we know the parameters of the two distributions, we can compute the parameters of the distribution of the sum.\n",
    "The following method shows how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55aea182",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_method_to Normal\n",
    "\n",
    "\n",
    "def __add__(self, other):\n",
    "    \"\"\"Distribution of the sum of two normal distribution.\"\"\"\n",
    "    return Normal(self.mu + other.mu, self.sigma2 + other.sigma2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c79e93",
   "metadata": {},
   "source": [
    "In the distribution of the sum, the mean is the sum of the means and the variance is the sum of the variances.\n",
    "Now that we've defined the special method `__add__`, we can use the `+` operator to \"add\" two distributions -- that is, to compute the distribution of their sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c113bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_sum = dist_male + dist_female\n",
    "dist_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44ddddb",
   "metadata": {},
   "source": [
    "To confirm that this result is correct, we'll use the following method, which plots the analytic CDF of a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b44c04a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_method_to Normal\n",
    "\n",
    "\n",
    "def plot_cdf(self, n_sigmas=3.5, **options):\n",
    "    \"\"\"Plot the CDF of this distribution.\"\"\"\n",
    "    mu, sigma = self.mu, np.sqrt(self.sigma2)\n",
    "    low, high = mu - n_sigmas * sigma, mu + n_sigmas * sigma\n",
    "    xs = np.linspace(low, high, 101)\n",
    "    ys = norm.cdf(xs, mu, sigma)\n",
    "    plt.plot(xs, ys, **options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040fe2fa",
   "metadata": {},
   "source": [
    "Here's the result along with the empirical CDF of the sum of the random samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a12aeec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_sum.plot_cdf(color=\"gray\", alpha=0.4, label=\"model\")\n",
    "Cdf.from_seq(sample_sum).plot(label=\"sample\")\n",
    "\n",
    "decorate(xlabel=\"Total weight (g)\", ylabel=\"CDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89ccf81",
   "metadata": {},
   "source": [
    "It looks like the parameters we computed are correct, which confirms that we can add two normal distributions by adding their means and variances.\n",
    "\n",
    "As a corollary, if we generate `n` values from a normal distribution and add them up, the distribution of the sum is also a normal distribution.\n",
    "To demonstrate, we'll start by generating 73 values from the distribution of male weights and adding them up.\n",
    "The following loop does that 1001 times, so the result is a sample from the distribution of sums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b338ce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(weights_male)\n",
    "sample_sums_male = [dist_male.sample(n).sum() for i in range(1001)]\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799c0137",
   "metadata": {},
   "source": [
    "The following method makes a `Normal` object that represents the distribution of the sums.\n",
    "To compute the parameters, we multiply both the mean and variance by `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9d0e16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_method_to Normal\n",
    "\n",
    "\n",
    "def sum(self, n):\n",
    "    \"\"\"Return the distribution of the sum of n values.\"\"\"\n",
    "    return Normal(n * self.mu, n * self.sigma2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394aef21",
   "metadata": {},
   "source": [
    "Here's the distribution of the sum of `n` weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dab094b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_sums_male = dist_male.sum(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92f17a0",
   "metadata": {},
   "source": [
    "And here's how it compares to the empirical distribution of the random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47a89b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_options = dict(color=\"gray\", alpha=0.4, label=\"model\")\n",
    "\n",
    "dist_sums_male.plot_cdf(**model_options)\n",
    "Cdf.from_seq(sample_sums_male).plot(label=\"sample\")\n",
    "\n",
    "decorate(xlabel=\"Total weights (g)\", ylabel=\"CDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5088c9c2",
   "metadata": {},
   "source": [
    "The analytic distribution fits the distribution of the sample, which confirms that the `sum` method is correct.\n",
    "So if we collect a sample of `n` measurements, we can compute the distribution of their sum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5d17c5",
   "metadata": {},
   "source": [
    "## Distribution of Sample Means\n",
    "\n",
    "If we can compute the distribution of a sample sum, we can also compute the distribution of a sample mean.\n",
    "To do that, we'll use a third property of a normal distribution: if we multiply or divide by a constant, the result is a normal distribution.\n",
    "The following methods show how we compute the parameters of the distribution of a product or quotient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30b72270",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_method_to Normal\n",
    "\n",
    "\n",
    "def __mul__(self, factor):\n",
    "    \"\"\"Multiplies by a scalar.\"\"\"\n",
    "    return Normal(factor * self.mu, factor**2 * self.sigma2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06428155",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_method_to Normal\n",
    "\n",
    "\n",
    "def __truediv__(self, factor):\n",
    "    \"\"\"Divides by a scalar.\"\"\"\n",
    "    return self * (1/factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e1fdc0",
   "metadata": {},
   "source": [
    "To compute the distribution of the product we multiply the mean by `factor` and the variance by the square of `factor`.\n",
    "We can use this property to compute the distribution of the sample means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6f5f3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mean_male = dist_sums_male / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e438275",
   "metadata": {},
   "source": [
    "To see if the result is correct, we'll also compute the means of the random samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a80243eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_means_male = np.array(sample_sums_male) / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68efbe8a",
   "metadata": {},
   "source": [
    "And compare the normal model to the empirical CDF of the sample means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d43572f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mean_male.plot_cdf(**model_options)\n",
    "Cdf.from_seq(sample_means_male).plot(label=\"sample\")\n",
    "\n",
    "decorate(xlabel=\"Average weight (g)\", ylabel=\"CDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f179135b",
   "metadata": {},
   "source": [
    "The model and the simulation results agree, which shows that we can compute the distribution of the sample means analytically -- which is very fast, compared to resampling.\n",
    "\n",
    "Now that we know the sampling distribution of the mean, we can use it to compute the standard error, which is the standard deviation of the sampling distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "288384ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_error = np.sqrt(dist_mean_male.sigma2)\n",
    "standard_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c59a783",
   "metadata": {},
   "source": [
    "This result suggests a shortcut we can use to compute the standard error directly, without computing the sampling distribution.\n",
    "In the sequence of steps we followed, we multiplied the variance by `n` and then divided by `n**2` -- the net effect was to divide the variance by `n`, which means we divided the standard deviation by the square root of `n`.\n",
    "\n",
    "So we can compute the standard error of the sample mean like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2ed43cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_error = weights_male.std() / np.sqrt(n)\n",
    "standard_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26c24d",
   "metadata": {},
   "source": [
    "This calculation is exact if the original distribution is normal, and approximate if the original distribution is approximately normal.\n",
    "We'll see why soon, but first let's consider one more result we can compute with normal distributions, the distribution of differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75191ea",
   "metadata": {},
   "source": [
    "## Distribution of Differences\n",
    "\n",
    "Putting together the steps from the previous section, here's how we can compute the distribution of sample means for the weights of the female penguins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dfb669f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(weights_female)\n",
    "dist_mean_female = dist_female.sum(n) / n\n",
    "dist_mean_female"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c427ef",
   "metadata": {},
   "source": [
    "Now we have sampling distributions for the average weight of male and female penguins -- let's compute the distribution of the differences.\n",
    "The following method computes the distribution of the difference between values from two normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ccc5b615",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_method_to Normal\n",
    "\n",
    "def __sub__(self, other):\n",
    "    \"\"\"Compute the distribution of a difference.\"\"\"\n",
    "    return Normal(self.mu - other.mu, self.sigma2 + other.sigma2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e41c7f",
   "metadata": {},
   "source": [
    "As you might expect, the mean of the differences is the difference of the means.\n",
    "But as you might not expect, the variance of the differences is not the difference of the variances -- it's the sum!\n",
    "To see why, imagine we perform subtraction in two steps:\n",
    "\n",
    "* If we negate the second distribution, the mean is negated but the variance is the same.\n",
    "\n",
    "* Then if we add in the first distribution, the variance of the sum is the sum of the variances.\n",
    "\n",
    "If that doesn't convince you, let's test it.\n",
    "Here's the analytic distribution of the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a5a76915",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_diff_means = dist_mean_male - dist_mean_female\n",
    "dist_diff_means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3298c6aa",
   "metadata": {},
   "source": [
    "And here's a random sample of differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce97b0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sums_female = [dist_female.sample(n).sum() for i in range(1001)]\n",
    "sample_means_female = np.array(sample_sums_female) / n\n",
    "sample_diff_means = sample_means_male - sample_means_female"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833be653",
   "metadata": {},
   "source": [
    "The following figure shows the empirical CDF of the random sample and the analytic CDF of the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a3c4db5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dist_diff_means.plot_cdf(**model_options)\n",
    "Cdf.from_seq(sample_diff_means).plot(label=\"sample\")\n",
    "\n",
    "decorate(xlabel=\"Difference in average weight (g)\", ylabel=\"CDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492d290f",
   "metadata": {},
   "source": [
    "They agree, which confirms that we found the distribution of the differences correctly. \n",
    "We can use this distribution to compute a confidence interval for the difference in weights.\n",
    "We'll use the following method to compute the \"percent point function\", which is the inverse CDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53cf66c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_method_to Normal\n",
    "\n",
    "def ppf(self, xs):\n",
    "    sigma = np.sqrt(self.sigma2)\n",
    "    return norm.ppf(xs, self.mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339941c6",
   "metadata": {},
   "source": [
    "The 5th and 95th percentiles form a 90% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "21d807a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ci90 = dist_diff_means.ppf([0.05, 0.95])\n",
    "ci90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57913169",
   "metadata": {},
   "source": [
    "We get approximately the same results from the random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5bcbbf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(sample_diff_means, [5, 95])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34c147b",
   "metadata": {},
   "source": [
    "The analytic method is faster than resampling, and it is deterministic -- that is, not random.\n",
    "\n",
    "However, everything we've done so far is based on the assumption that the distribution of measurements is normal.\n",
    "That's not always true -- in fact, with real data it is never exactly true.\n",
    "But even if the distribution of the measurements isn't normal, if we add up many measurements, the distribution of their sum is often close to normal.\n",
    "That is the power of the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a0241c",
   "metadata": {},
   "source": [
    "## Central Limit Theorem\n",
    "\n",
    "As we saw in the previous sections, if we add values drawn from normal distributions, the distribution of the sum is normal.\n",
    "Most other distributions don't have this property -- for example, if we add values drawn from an exponential distribution, the distribution of the sum is not exponential.\n",
    "\n",
    "But for many distributions, if we generate `n` values and add them up, the distribution of the sum converges to normal as `n` increases.\n",
    "More specifically, if the distribution of the values has mean `m` and variance `s2` the distribution of the sum converges to a normal distribution with mean `n * m` and variance `n * s2`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d7def3",
   "metadata": {},
   "source": [
    "That conclusion is the Central Limit Theorem (CLT).\n",
    "It is one of the most useful tools for statistical analysis, but it comes with caveats:\n",
    "\n",
    "-   The values have to come from the same distribution (although this requirement can be relaxed).\n",
    "\n",
    "-   The values have to be drawn independently. If they are correlated, the CLT doesn't apply (although it can still work if the correlation is not too strong).\n",
    "\n",
    "-   The values have to be drawn from a distribution with finite mean and variance. So the CLT doesn't apply to some long-tailed distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64651d4",
   "metadata": {},
   "source": [
    "The Central Limit Theorem explains the prevalence of normal distributions in the natural world.\n",
    "Many characteristics of living things are affected by genetic and environmental factors whose effect is additive.\n",
    "The characteristics we measure are the sum of a large number of small effects, so their distribution tends to be normal.\n",
    "\n",
    "To see how the Central Limit Theorem works, and when it doesn't, let's try some experiments,\n",
    "starting with an exponential distribution.\n",
    "The following loop generates samples from an exponential distribution, adds them up, and makes a dictionary that maps from each sample size, `n`, to a list of 1001 sums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c3c5b9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 1\n",
    "df_sample_expo = pd.DataFrame()\n",
    "for n in [1, 10, 100]:\n",
    "    df_sample_expo[n] = [np.sum(np.random.exponential(lam, n)) for _ in range(1001)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6d3d30",
   "metadata": {},
   "source": [
    "Here are the averages for each list of sums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c03e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_expo.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0231880",
   "metadata": {},
   "source": [
    "The average value from this distribution is 1, so if we add up 10 values, the average of the sum is close to 10, and if we add up 100 values the average of the sum is close to 100.\n",
    "\n",
    "Now let's see what the distributions of these sums look like.\n",
    "This function takes the `DataFrame` we just made and makes a normal probability plot for each list of sums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8f957be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_plot_samples(df_sample, ylabel=\"\"):\n",
    "    \"\"\"Normal probability plots for samples of sums.\"\"\"\n",
    "    plt.figure(figsize=(8, 3.5))\n",
    "    for i, n in enumerate(df_sample):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        normal_probability_plot(df_sample[n])\n",
    "        decorate(\n",
    "            title=\"n=%d\" % n,\n",
    "            xticks=[],\n",
    "            yticks=[],\n",
    "            xlabel=\"Standard normal\",\n",
    "            ylabel=ylabel,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42166062",
   "metadata": {},
   "source": [
    "Here are the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8955b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_plot_samples(df_sample_expo, ylabel=\"Sum of exponential values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a347e4e",
   "metadata": {},
   "source": [
    "When `n=1`, the distribution of the sum is exponential, so the normal probability plot is not a straight line.\n",
    "But with `n=10` the distribution of the sum is approximately normal, and with `n=100` it is almost indistinguishable from normal.\n",
    "\n",
    "For distributions that are less skewed than an exponential, the distribution of the sum converges to normal more quickly -- that is, for smaller values of `n`.\n",
    "For distributions that are more skewed, it takes longer.\n",
    "As an example, let's look at sums of values from a lognormal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3f2234ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 1.0, 1.0\n",
    "df_sample_lognormal = pd.DataFrame()\n",
    "for n in [1, 10, 100]:\n",
    "    df_sample_lognormal[n] = [\n",
    "        np.sum(np.random.lognormal(mu, sigma, n)) for _ in range(1001)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b97114",
   "metadata": {},
   "source": [
    "Here are the normal probability plots for the same range of sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7a807d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_plot_samples(df_sample_lognormal, ylabel=\"Sum of lognormal values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ef356c",
   "metadata": {},
   "source": [
    "When `n=1`, a normal model does not fit the distribution, and it is not much better with `n=10`.\n",
    "Even with `n=100`, the tails of the distribution clearly deviate from the model.\n",
    "\n",
    "The mean and variance of the lognormal distribution are finite, so the distribution of the sum converges to normal eventually.\n",
    "But for some highly skewed distributions, it might not converge at any practical sample size.\n",
    "And in some cases, it doesn't happen at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f12f6e",
   "metadata": {},
   "source": [
    "## The Limits of the Central Limit Theorem\n",
    "\n",
    "Pareto distributions are even more skewed than lognormal.\n",
    "Depending on the parameters, some Pareto distributions do not have finite mean and variance -- in those cases, the Central Limit Theorem does not apply.\n",
    "\n",
    "To demonstrate, we'll generate values from a Pareto distribution with `alpha=1`, which has infinite mean and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f98ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.0\n",
    "df_sample = pd.DataFrame()\n",
    "for n in [1, 10, 100]:\n",
    "    df_sample[n] = [np.sum(np.random.pareto(alpha, n)) for _ in range(1001)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869e2345",
   "metadata": {},
   "source": [
    "Here's what the normal probability plots look like for a range of sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5a039a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_plot_samples(df_sample, ylabel=\"Sum of Pareto values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cec0afd",
   "metadata": {},
   "source": [
    "Even with `n=100`, the distribution of the sum is nothing like a normal distribution.\n",
    "\n",
    "I also mentioned that the CLT does not apply if the values are correlated.\n",
    "To test that, we'll use the following function, which generates values from a normal distribution where the serial correlation -- that is, the correlation between successive elements in the sample -- is the given value, `rho`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7bf8b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_normal_correlated(n, rho):\n",
    "    \"\"\"Generates an array of correlated values from a standard normal dist.\"\"\"\n",
    "    xs = np.empty(n)\n",
    "    xs[0] = np.random.normal(0, 1)\n",
    "\n",
    "    sigma = np.sqrt(1 - rho**2)\n",
    "    for i in range(1, n):\n",
    "        xs[i] = rho * xs[i - 1] + np.random.normal(0, sigma)\n",
    "\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060814fb",
   "metadata": {},
   "source": [
    "Given a correlated sequence from a normal distribution, the following function generates a correlated sequence from an exponential distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e0de13ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon\n",
    "\n",
    "\n",
    "def generate_expo_correlated(n, rho):\n",
    "    \"\"\"Generates a sequence of correlated values from an exponential dist.\"\"\"\n",
    "    normal = generate_normal_correlated(n, rho)\n",
    "    uniform = norm.cdf(normal)\n",
    "    expo = expon.ppf(uniform)\n",
    "    return expo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa600477",
   "metadata": {},
   "source": [
    "It starts with a sequence of correlated normal values and uses the normal CDF to transform them to a sequence of values from a uniform distribution between 0 and 1.\n",
    "Then it uses the exponential inverse CDF to transform them to a sequence of exponential values.\n",
    "\n",
    "Again can make a `DataFrame` with one column for each sample size and 1001 sums in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9054d1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = 0.8\n",
    "df_sample = pd.DataFrame()\n",
    "for n in [1, 10, 100]:\n",
    "    df_sample[n] = [np.sum(generate_expo_correlated(n, rho)) for _ in range(1001)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff357a",
   "metadata": {},
   "source": [
    "Here are the normal probability plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "be3de5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_plot_samples(df_sample, ylabel=\"sum of correlated exponential values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e44ca00",
   "metadata": {},
   "source": [
    "With `rho=0.8`, there is a strong correlation between successive elements, and the distribution of the sum converges slowly.\n",
    "If there is also a strong correlation between distant elements of the sequence, it might not converge at all.\n",
    "\n",
    "These experiments are meant to show how the Central Limit Theorem works, and what happens when it doesn't.\n",
    "Now let's see how we can use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1093a914",
   "metadata": {},
   "source": [
    "## Applying the CLT\n",
    "\n",
    "To see why the Central Limit Theorem is useful, let's get back to the example in Section XXX: testing the apparent difference in mean pregnancy length for first babies and others.\n",
    "Instructions for downloading the NSFG data are in the notebook for this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a245eae",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following cell downloads the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eb1410e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "download(\"https://github.com/AllenDowney/ThinkStats/raw/v3/nb/nsfg.py\")\n",
    "download(\"https://github.com/AllenDowney/ThinkStats/raw/v3/data/2002FemPreg.dct\")\n",
    "download(\"https://github.com/AllenDowney/ThinkStats/raw/v3/data/2002FemPreg.dat.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6532d8",
   "metadata": {},
   "source": [
    "We'll use `get_nsfg_groups` to read the data and divide it into first babies and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0e95be2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nsfg import get_nsfg_groups\n",
    "\n",
    "live, firsts, others = get_nsfg_groups()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa118107",
   "metadata": {},
   "source": [
    "As we've seen, first babies are born a little later, on average -- the apparent difference is about 0.078 weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6564a11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = firsts[\"prglngth\"].mean() - others[\"prglngth\"].mean()\n",
    "delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4677bb23",
   "metadata": {},
   "source": [
    "Now, to see whether this difference might have happened by chance, we'll assume as a null hypothesis that the mean and variance of pregnancy lengths is actually the same for both groups, so we can estimate it using all live births."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b7cd0412",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lengths = live[\"prglngth\"]\n",
    "m, s2 = all_lengths.mean(), all_lengths.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e52e723",
   "metadata": {},
   "source": [
    "The distribution of pregnancy lengths does not follow a normal distribution -- nevertheless, we can use a normal distribution to approximate the sampling distribution of the mean.\n",
    "\n",
    "The following function takes a sequence of values and returns a `Normal` object that represents the sampling distribution of the mean of a sample with the given size, `n`, drawn from a normal distribution with the same mean and variance as the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fd578a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_dist_mean(data, n):\n",
    "    mean, var = data.mean(), data.var()\n",
    "    dist = Normal(mean, var)\n",
    "    return dist.sum(n) / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cbfbec",
   "metadata": {},
   "source": [
    "Here's a normal approximation to the sampling distribution of mean weight for first births, under the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c4dee8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = firsts[\"totalwgt_lb\"].count()\n",
    "dist_firsts = sampling_dist_mean(all_lengths, n1)\n",
    "n1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce02922",
   "metadata": {},
   "source": [
    "And here's the sampling distribution for other babies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b3a1c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n2 = others[\"totalwgt_lb\"].count()\n",
    "dist_others = sampling_dist_mean(all_lengths, n2)\n",
    "n2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e712960d",
   "metadata": {},
   "source": [
    "We can compute the sampling distribution of the difference like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "750e153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_diff = dist_firsts - dist_others\n",
    "dist_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e57085",
   "metadata": {},
   "source": [
    "The mean is 0, which makes sense because if we draw two samples from the same distribution, we expect the difference in means to be 0, on average.\n",
    "The variance of the sampling distribution is 0.0032, which indicates how much variation we expect in the differences due to chance.\n",
    "\n",
    "To confirm that this distribution approximates the sampling distribution, we can also estimate it by resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "abc82799",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_firsts = [np.random.choice(all_lengths, n1).mean() for i in range(1001)]\n",
    "sample_others = [np.random.choice(all_lengths, n2).mean() for i in range(1001)]\n",
    "sample_diffs = np.subtract(sample_firsts, sample_others)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e2a274",
   "metadata": {},
   "source": [
    "Here's the empirical CDF of the resampled differences compared to the normal model.\n",
    "The vertical dotted lines show the observed difference, positive and negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "abbbe574",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_diff.plot_cdf(**model_options)\n",
    "Cdf.from_seq(sample_diffs).plot(label=\"sample\")\n",
    "plt.axvline(delta, ls=\":\")\n",
    "plt.axvline(-delta, ls=\":\")\n",
    "\n",
    "decorate(xlabel=\"Difference in pregnancy length\", ylabel=\"CDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d22318",
   "metadata": {},
   "source": [
    "In this example, the sample sizes are large and the skewness of the measurements is modest, so the sampling distribution is well approximated by a normal distribution.\n",
    "Therefore, we can use the normal CDF to compute a p-value.\n",
    "The following method computes the CDF of a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0c328609",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_method_to Normal\n",
    "\n",
    "def cdf(self, xs):\n",
    "    sigma = np.sqrt(self.sigma2)\n",
    "    return norm.cdf(xs, self.mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95cd9ca",
   "metadata": {},
   "source": [
    "Here's the probability of a difference as large as `delta` under the null hypothesis, which is the right tail of the sampling distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "74bf6c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "right = 1 - dist_diff.cdf(delta)\n",
    "right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558c5533",
   "metadata": {},
   "source": [
    "And here's the probability of a difference as small as `-delta`, which is the left tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "356d9768",
   "metadata": {},
   "outputs": [],
   "source": [
    "left = dist_diff.cdf(-delta)\n",
    "left"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f8b49e",
   "metadata": {},
   "source": [
    "`left` and `right` are the same because the normal distribution is symmetric.\n",
    "The sum of the two is the probability of a difference as large as `delta`, positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9cc48212",
   "metadata": {},
   "outputs": [],
   "source": [
    "left + right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c262b4",
   "metadata": {},
   "source": [
    "The resulting p-value is 0.168, which is consistent with the estimate we computed by resampling in Section XXX.\n",
    "\n",
    "The way we computed this p-value is similar to an **independent sample $t$ test**.\n",
    "SciPy provides a function called `ttest_ind` that takes two samples and computes a p-value for the difference in their means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aca62308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "result = ttest_ind(firsts[\"prglngth\"], others[\"prglngth\"])\n",
    "result.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a41a935",
   "metadata": {},
   "source": [
    "When the sample sizes are large, the result of the $t$ test is close to what we computed with normal distributions.\n",
    "The $t$ test is so called because it is based on Student's $t$ distribution rather than a normal distribution.\n",
    "The $t$ distribution is also useful for testing whether a correlation is statistically significant, as we'll see in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a90067",
   "metadata": {},
   "source": [
    "## Correlation Test\n",
    "\n",
    "In Section xxx we used a permutation test for the correlation between birth weight and mother's age, and found that it is statistically significant, with p-value less than 0.001.\n",
    "\n",
    "Now we can do the same thing analytically.\n",
    "The method is based on this mathematical result: If we generate two samples with size `n` from normal distributions, compute Pearson's correlation, `r`, and then transform the correlation with this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f093f5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_correlation(r, n):\n",
    "    return r * np.sqrt((n - 2) / (1 - r**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1082fa",
   "metadata": {},
   "source": [
    "The transformed correlations follow a $t$ distribution with parameter $n-2$.\n",
    "To see what that looks like, we'll use the following function to generate uncorrelated samples from a standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1cbd4f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n):\n",
    "    \"\"\"Uncorrelated sequences from a standard normal.\"\"\"\n",
    "    xs = np.random.normal(0, 1, n)\n",
    "    ys = np.random.normal(0, 1, n)\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64d42ec",
   "metadata": {},
   "source": [
    "And the following function to compute their correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "459d2014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(data):\n",
    "    xs, ys = data\n",
    "    return np.corrcoef(xs, ys)[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0e089a",
   "metadata": {},
   "source": [
    "The following loop generates many pairs of samples, computes their correlation, and puts the results in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bcc51c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "rs = [correlation(generate_data(n)) for i in range(1001)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3bed9a",
   "metadata": {},
   "source": [
    "Next we'll compute the transformed correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0838a9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = transform_correlation(np.array(rs), n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f185627",
   "metadata": {},
   "source": [
    "To check whether these `ts` follow a $t$ distribution, we'll use the following function, which makes a `Cdf` object that represents the CDF of a $t$ distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a5ff7e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t as student_t\n",
    "\n",
    "\n",
    "def make_student_cdf(df):\n",
    "    \"\"\"Computes the CDF of a Student t distributions.\"\"\"\n",
    "    ts = np.linspace(-3, 3, 101)\n",
    "    ps = student_t.cdf(ts, df=df)\n",
    "    return Cdf(ps, ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62adbbfc",
   "metadata": {},
   "source": [
    "The parameter of the $t$ distribution is called `df`, which stands for \"degrees of freedom\".\n",
    "The following figure shows the CDF of a $t$ distribution with parameter `n-2` along with the empirical CDF of the transformed correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "433b9e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_student_cdf(df=n - 2).plot(**model_options)\n",
    "\n",
    "cdf_ts = Cdf.from_seq(ts)\n",
    "cdf_ts.plot(label=\"random normals\")\n",
    "\n",
    "decorate(xlabel=\"Transformed correlation\", ylabel=\"CDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9f8866",
   "metadata": {},
   "source": [
    "This shows that if we draw uncorrelated samples from normal distributions, their transformed correlations follow a $t$ distribution.\n",
    "\n",
    "If we draw samples from other distributions, their transformed correlations don't follow a $t$ distribution exactly, but they converge to a $t$ distribution as the sample size increases.\n",
    "Let's see if this applies to the correlation of maternal age and birth weight.\n",
    "From the `DataFrame` of live births, we'll select the rows with valid data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8bff6f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = live.dropna(subset=[\"agepreg\", \"totalwgt_lb\"])\n",
    "n = len(valid)\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e8beb7",
   "metadata": {},
   "source": [
    "The actual correlation is about 0.07."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "af2b5f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = valid[\"agepreg\"].values, valid[\"totalwgt_lb\"].values\n",
    "r_actual = correlation(data)\n",
    "r_actual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e159ce",
   "metadata": {},
   "source": [
    "As we did in Section xxx, we can simulate the null hypothesis by permuting the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a3c281f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute(data):\n",
    "    \"\"\"Shuffle the x values.\"\"\"\n",
    "    xs, ys = data\n",
    "    new_xs = xs.copy()\n",
    "    np.random.shuffle(new_xs)\n",
    "    return new_xs, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073c8bd3",
   "metadata": {},
   "source": [
    "If we generate many permutations and compute their correlations, the result is a sample from the distribution of correlations under the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8d901850",
   "metadata": {},
   "outputs": [],
   "source": [
    "permuted_corrs = [correlation(permute(data)) for i in range(1001)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135fc475",
   "metadata": {},
   "source": [
    "And we can compute the transformed correlations like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "167b4847",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = transform_correlation(np.array(permuted_corrs), n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77383d1d",
   "metadata": {},
   "source": [
    "The following figure shows the empirical CDF of the `ts` along with the CDF of the $t$ distribution with parameter `n-2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6bd62d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_student_cdf(n - 2).plot(**model_options)\n",
    "Cdf.from_seq(ts).plot(label=\"permuted data\")\n",
    "\n",
    "decorate(xlabel=\"Transformed correlation\", ylabel=\"CDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e529ac6",
   "metadata": {},
   "source": [
    "The model fits the empirical distribution well, which means we can use it to compute a p-value for the observed correlation.\n",
    "First we'll transform the observed correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2aa20fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_actual = transform_correlation(r_actual, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baa82fa",
   "metadata": {},
   "source": [
    "Now we can use the CDF of the $t$ distribution to compute the probability of a value as large as `t_actual` under the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "561a79d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "right = 1 - student_t.cdf(t_actual, df=n - 2)\n",
    "right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c27265",
   "metadata": {},
   "source": [
    "We can also compute the probability of a value as small as `-t_actual`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e52caba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "left = student_t.cdf(-t_actual, df=n - 2)\n",
    "left"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7984129",
   "metadata": {},
   "source": [
    "The sum of the two is the probability of a correlation as big as `r_actual`, positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "32a943e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "left + right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff9bf61",
   "metadata": {},
   "source": [
    "SciPy provides a function that does the same calculation and returns the p-value of the observed correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "612bccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "corr, p_value = pearsonr(*data)\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d80df7d",
   "metadata": {},
   "source": [
    "Based on the resampling results, we concluded that the p-value was less than 0.001, but we could not say how much less without running a very large number of resamplings.\n",
    "With analytic methods, we can compute small p-values quickly.\n",
    "\n",
    "However, in practice it might not matter.\n",
    "Generally, if a p-value is smaller than 0.001, we can conclude that the observed effect is unlikely to be due to chance.\n",
    "It is usually not important to know precisely how unlikely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29e5091",
   "metadata": {},
   "source": [
    "## Chi-squared Test\n",
    "\n",
    "In Section XXX we tested whether a die is crooked, based on this set of observed outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5b66ea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from empiricaldist import Hist\n",
    "\n",
    "qs = np.arange(1, 7)\n",
    "freqs = [8, 9, 19, 5, 8, 11]\n",
    "observed = Hist(freqs, qs)\n",
    "observed.index.name = \"outcome\"\n",
    "observed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eb0c94",
   "metadata": {},
   "source": [
    "First we computed the expected frequency for each outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7e6e0511",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rolls = observed.sum()\n",
    "outcomes = observed.qs\n",
    "expected = Hist(num_rolls / 6, outcomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a104b3e1",
   "metadata": {},
   "source": [
    "Then we used the following function to compute the chi-squared statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e170a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_squared_stat(observed, expected):\n",
    "    diffs = (observed - expected) ** 2\n",
    "    ratios = diffs / expected\n",
    "    return np.sum(ratios.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6f87b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_chi2 = chi_squared_stat(observed, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eea3ee6",
   "metadata": {},
   "source": [
    "The chi-squared statistic is widely used for this kind of data because its sampling distribution under the null hypothesis converges to a distribution we can compute efficiently -- not coincidentally, it is called a **chi-squared distribution**.\n",
    "To see what it looks like, we'll use the following function, which simulates rolling a fair die."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8d3a4e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_dice(observed):\n",
    "    n = np.sum(observed)\n",
    "    rolls = np.random.choice(observed.qs, n, replace=True)\n",
    "    hist = Hist.from_seq(rolls)\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122f792d",
   "metadata": {},
   "source": [
    "The following loop runs the simulation many times and computes the chi-squared statistic of the outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "21c97c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_chi_squared = [\n",
    "    chi_squared_stat(simulate_dice(observed), expected) for i in range(1001)\n",
    "]\n",
    "cdf_simulated = Cdf.from_seq(simulated_chi_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2806c8",
   "metadata": {},
   "source": [
    "To check whether the results follow a chi-squared distribution, we'll use the following function, which computes the CDF of a chi-squared distribution with parameter `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3e2be4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2 as chi2_dist\n",
    "\n",
    "\n",
    "def chi_squared_cdf(df):\n",
    "    \"\"\"Discrete approximation of the chi-squared CDF with df=n-1.\n",
    "\n",
    "    n: sample size\n",
    "\n",
    "    returns: Cdf\n",
    "    \"\"\"\n",
    "    xs = np.linspace(0, 21, 101)\n",
    "    ps = chi2_dist.cdf(xs, df=df)\n",
    "    return Cdf(ps, xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66f4fd7",
   "metadata": {},
   "source": [
    "With `n` possible outcomes, the simulated chi-squared statistics should follow a chi-squared distibution with parameter `n-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a4801a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(observed)\n",
    "cdf_model = chi_squared_cdf(df=n - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84864e4",
   "metadata": {},
   "source": [
    "Here's the empirical CDF of the simulated chi-squared statistics along with the CDF of the chi-squared distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aca6b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_model.plot(**model_options)\n",
    "cdf_simulated.plot(label=\"simulation\")\n",
    "\n",
    "decorate(xlabel=\"Chi-squared statistic\", ylabel=\"CDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9789d9",
   "metadata": {},
   "source": [
    "The model fits the simulation results well, so we can use it to compute the probability of a value as large as `observed_chi2` under the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "761861dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = 1 - chi2_dist.cdf(observed_chi2, df=n - 1)\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844c297f",
   "metadata": {},
   "source": [
    "SciPy provides a function that does the same computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e924fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chisquare\n",
    "\n",
    "chi2_stat, p_value = chisquare(f_obs=observed, f_exp=expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cbe47e",
   "metadata": {},
   "source": [
    "The result is the same as the p-value we computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f403b881",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158635d2",
   "metadata": {},
   "source": [
    "The advantage of the chi-squared statistic is that its distribution under the null hypothesis can be computed efficiently.\n",
    "But it might not be the statistic that best quantifies the difference between the observed and expected outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffdd14a",
   "metadata": {},
   "source": [
    "## Computation and Analysis\n",
    "\n",
    "This book focuses on computational methods like resampling and permutation.\n",
    "These methods have several advantages over analysis:\n",
    "\n",
    "-   They are easier to explain and understand. For example, one of the most difficult topics in an introductory statistics class is hypothesis testing. Many students don't really understand what p-values are. I think the approach we took in Chapter xxx -- simulating the null hypothesis and computing test statistics -- makes the fundamental idea clearer.\n",
    "\n",
    "-   They are robust and versatile. Analytic methods are often based on assumptions that don't hold in practice. Computational methods require fewer assumptions, and can be adapted and extended more easily.\n",
    "\n",
    "-   They are debuggable. Analytic methods are often like a black box: you plug in numbers and they spit out results. But it's easy to make subtle errors, hard to be confident that the results are right, and hard to find the problem if they are not. Computational methods lend themselves to incremental development and testing, which fosters confidence in the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a568fca",
   "metadata": {},
   "source": [
    "But there is one drawback: computational methods can be slow.\n",
    "Taking into account these pros and cons, I recommend the following process:\n",
    "\n",
    "1.  Use computational methods during exploration. If you find a satisfactory answer and the run time is acceptable, you can stop.\n",
    "\n",
    "2.  If run time is not acceptable, look for opportunities to optimize. Using analytic methods is one of several methods of optimization.\n",
    "\n",
    "3.  If replacing a computational method with an analytic method is appropriate, use the computational method as a basis of comparison, providing mutual validation between the computational and analytic results.\n",
    "\n",
    "For many practical problems, the run time of computational methods is not a problem, and we don't have to go past the first step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e9a0da",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4273899",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "In this chapter we compared the weights of male and female penguins and computes a confidence interval for the difference. Now let's do the same for flipper length.\n",
    "The observed difference is about 4.6 mm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "90ea3112",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = adelie.groupby(\"Sex\")\n",
    "\n",
    "lengths_male = grouped.get_group(\"MALE\")[\"Flipper Length (mm)\"]\n",
    "lengths_female = grouped.get_group(\"FEMALE\")[\"Flipper Length (mm)\"]\n",
    "observed_diff = lengths_male.mean() - lengths_female.mean()\n",
    "observed_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b740ff",
   "metadata": {},
   "source": [
    "Use `sampling_dist_mean` to make `Normal` objects that represent sampling distributions for the mean flipper length in the two groups -- noting that the groups are not the same size.\n",
    "Then compute the sampling distribution of the difference and a 90% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "75114d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c05bd9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c63f5104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "aeba3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b6425b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f16c8a",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Using the NSFG data, we computed the correlation between a baby's birth weight and the mother's age, and we used a $t$ distribution to compute a p-value.\n",
    "Now let's do the same with birth weight and father's age, which is recorded in the `hpagelb` column.\n",
    "The observed correlation is about 0.065."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "609ac7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = live.dropna(subset=[\"hpagelb\", \"totalwgt_lb\"])\n",
    "n = len(valid)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2498abe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = valid[\"hpagelb\"].values, valid[\"totalwgt_lb\"].values\n",
    "r_actual = correlation(data)\n",
    "r_actual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b9435e",
   "metadata": {},
   "source": [
    "Compute the transformed correlation, `t_actual`.\n",
    "Use the CDF of the $t$ distribution to compute a p-value -- is this correlation statistically significant?\n",
    "Use the SciPy function `pearsonr` to check your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a1f97e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7a927896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "af04a9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38991254",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "In one of the exercises in Chapter xxx we considered the Trivers-Willard hypothesis, which suggests that for many mammals the sex ratio depends on \"maternal condition\" -- that is, factors like the mother's age, size, health, and social status.\n",
    "Some studies have shown this effect among humans, but results are mixed.\n",
    "\n",
    "As an example -- and a chance to practice a chi-squared test -- let's see if there's a relationship between the sex of a baby and the mothers marital status.\n",
    "The notebook for this chapter has instructions to help you get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a666cc01",
   "metadata": {
    "tags": []
   },
   "source": [
    "First we'll partition mothers of male and female babies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c8fde0a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "male = live.query(\"babysex == 1\")\n",
    "female = live.query(\"babysex == 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79253118",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we'll make a `DataFrame` with one column for each group and one row for each value of `fmarital`, which encodes marital status like this:\n",
    "\n",
    "```\n",
    "1  married\n",
    "2  widowed\n",
    "3  divorces\n",
    "4  separated\n",
    "5  never married\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3518a7f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "observed = pd.DataFrame()\n",
    "observed[\"male\"] = male[\"fmarital\"].value_counts().sort_index()\n",
    "observed[\"female\"] = female[\"fmarital\"].value_counts().sort_index()\n",
    "observed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7928f300",
   "metadata": {
    "tags": []
   },
   "source": [
    "The null hypothesis is that the distribution of marital status is the same for both groups, so we can use the whole dataset to compute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "005067e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from empiricaldist import Pmf\n",
    "\n",
    "pmf_fmarital = Pmf.from_seq(live[\"fmarital\"])\n",
    "pmf_fmarital"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a996752f",
   "metadata": {
    "tags": []
   },
   "source": [
    "To compute the expected values, we multiply the probabilities in `pmf_marital` by the total number of cases in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "67b30bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "expected = pd.DataFrame()\n",
    "expected[\"male\"] = pmf_fmarital * observed[\"male\"].sum()\n",
    "expected[\"female\"] = pmf_fmarital * observed[\"female\"].sum()\n",
    "expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ba5492",
   "metadata": {
    "tags": []
   },
   "source": [
    "Use `observed` and `expected` to compute a chi-squared statistic.\n",
    "Then use the CDF of the chi-squared distribution to compute a p-value.\n",
    "The degrees of freedom should be `n-1`, where `n` is the number of values in the observed `DataFrame`.\n",
    "Then use the SciPy function `chisquared` to compute the chi-squared statistic and p-value.\n",
    "Hint: use the argument `axis=None` to treat the entire `DataFrame` as a single test rather than one test for each column.\n",
    "\n",
    "Does this test provide support for the Trivers-Willard hypothesis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3c584452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fbe52b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d82cc9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e7d15cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "33e34fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4d4102",
   "metadata": {},
   "source": [
    "**Exercise:** The method we used in this chapter to analyze differences between groups can be extended to analyze \"differences in differences\", which is a common experimental design.\n",
    "As an example, we'll use data from a 2014 paper that investigates the effects of an intervention intended to mitigate gender-stereotypical task allocation within student engineering teams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8573a5be",
   "metadata": {
    "tags": []
   },
   "source": [
    "Stein, L. A., Aragon, D., Moreno, D., & Goodman, J. (2014, October). Evidence for the persistent effects of an intervention to mitigate gender-stereotypical task allocation within student engineering teams. In *2014 IEEE Frontiers in Education Conference (FIE) Proceedings* (pp. 1-9). IEEE.\n",
    "\n",
    "Available from <http://ieeexplore.ieee.org/document/7044435/>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8623c0d",
   "metadata": {},
   "source": [
    "Before and after the intervention, students responded to a survey that asked them to rate their contribution to each aspect of class projects on a 7-point scale.\n",
    "\n",
    "Before the intervention, male students reported higher scores for the programming aspect of the projects than female students: men reported an average score of 3.57 with standard error 0.28; women reported an average score of 1.91 with standard error 0.32.\n",
    "\n",
    "After the intervention, the gender gap was smaller: the average score for men was 3.44 (SE 0.16); the average score for women was 3.18 (SE 0.16).\n",
    "\n",
    "1. Make `Normal` objects that represent the sampling distributions of the estimated means before and after the intervention. Because we have standard errors for the estimated means, we don't need to know the sample size to figure out the sampling distributions.\n",
    "\n",
    "2. Compute the sampling distributions of the gender gap -- the difference in means -- before and after the intervention.\n",
    "\n",
    "3. Then compute the sampling distribution of the difference in differences -- that is, the change in the size of the gap. Compute a 95% confidence interval and a p-value.\n",
    "\n",
    "Is there evidence that the size of the gender gap decreased after the intervention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ef290a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "327daf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "feb8e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d3476d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "834ffcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27da3a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "[Think Stats: Exploratory Data Analysis in Python, 3rd Edition](https://allendowney.github.io/ThinkStats/index.html)\n",
    "\n",
    "Copyright 2024 [Allen B. Downey](https://allendowney.com)\n",
    "\n",
    "Code license: [MIT License](https://mit-license.org/)\n",
    "\n",
    "Text license: [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International](https://creativecommons.org/licenses/by-nc-sa/4.0/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5d3c15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
