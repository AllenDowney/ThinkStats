{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79cfee17",
   "metadata": {},
   "source": [
    "# Relationships between variables\n",
    "\n",
    "So far we have only looked at one variable at a time. In this chapter we\n",
    "look at relationships between variables. Two variables are related if\n",
    "knowing one gives you information about the other. For example, height\n",
    "and weight are related; people who are taller tend to be heavier. Of\n",
    "course, it is not a perfect relationship: there are short heavy people\n",
    "and tall light ones. But if you are trying to guess someone's weight,\n",
    "you will be more accurate if you know their height than if you don't.\n",
    "\n",
    "The code for this chapter is in `scatter.py`. For information about\n",
    "downloading and working with this code, see\n",
    "Section [\\[code\\]](#code){reference-type=\"ref\" reference=\"code\"}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b39cb1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "542c6943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import basename, exists\n",
    "\n",
    "\n",
    "def download(url):\n",
    "    filename = basename(url)\n",
    "    if not exists(filename):\n",
    "        from urllib.request import urlretrieve\n",
    "\n",
    "        local, _ = urlretrieve(url, filename)\n",
    "        print(\"Downloaded \" + local)\n",
    "\n",
    "\n",
    "download(\"https://github.com/AllenDowney/ThinkStats/raw/v3/nb/thinkstats2.py\")\n",
    "download(\"https://github.com/AllenDowney/ThinkStats/raw/v3/nb/thinkplot.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ca7f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import thinkstats2\n",
    "import thinkplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df42f753",
   "metadata": {},
   "source": [
    "## Scatter plots\n",
    "\n",
    "The simplest way to check for a relationship between two variables is a\n",
    "**scatter plot**, but making a good scatter plot is not always easy. As\n",
    "an example, I'll plot weight versus height for the respondents in the\n",
    "BRFSS (see Section [\\[lognormal\\]](#lognormal){reference-type=\"ref\"\n",
    "reference=\"lognormal\"}).\n",
    "\n",
    "Here's the code that reads the data file and extracts height and weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b344c927",
   "metadata": {},
   "outputs": [],
   "source": [
    "download(\"https://github.com/AllenDowney/ThinkStats/raw/v3/nb/brfss.py\")\n",
    "download(\"https://github.com/AllenDowney/ThinkStats/raw/v3/nb/CDBRFS08.ASC.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0d79648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brfss\n",
    "\n",
    "df = brfss.read_brfss(nrows=None)\n",
    "sample = thinkstats2.sample_rows(df, 5000)\n",
    "heights, weights = sample.htm3, sample.wtkg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697262c7",
   "metadata": {},
   "source": [
    "`SampleRows` chooses a random subset of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cdcbcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_rows(df, nrows, replace=False):\n",
    "    indices = np.random.choice(df.index, nrows, replace=replace)\n",
    "    sample = df.loc[indices]\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1df355d",
   "metadata": {},
   "source": [
    "`df` is the DataFrame, `nrows` is the number of rows to choose, and\n",
    "`replace` is a boolean indicating whether sampling should be done with\n",
    "replacement; in other words, whether the same row could be chosen more\n",
    "than once.\n",
    "\n",
    "`thinkplot` provides `Scatter`, which makes scatter plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da4cfee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "thinkplot.scatter(heights, weights)\n",
    "thinkplot.show(xlabel=\"Height (cm)\", ylabel=\"Weight (kg)\", axis=[140, 210, 20, 200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101eda69",
   "metadata": {},
   "source": [
    "The result, in Figure [\\[scatter1\\]](#scatter1){reference-type=\"ref\"\n",
    "reference=\"scatter1\"} (left), shows the shape of the relationship. As we\n",
    "expected, taller people tend to be heavier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cf0b5f",
   "metadata": {},
   "source": [
    "But this is not the best representation of the data, because the data\n",
    "are packed into columns. The problem is that the heights are rounded to\n",
    "the nearest inch, converted to centimeters, and then rounded again. Some\n",
    "information is lost in translation.\n",
    "\n",
    "We can't get that information back, but we can minimize the effect on\n",
    "the scatter plot by **jittering** the data, which means adding random\n",
    "noise to reverse the effect of rounding off. Since these measurements\n",
    "were rounded to the nearest inch, they might be off by up to 0.5 inches\n",
    "or 1.3 cm. Similarly, the weights might be off by 0.5 kg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30e7bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "heights = thinkstats2.jitter(heights, 1.3)\n",
    "weights = thinkstats2.jitter(weights, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eb8efd",
   "metadata": {},
   "source": [
    "Here's the implementation of `Jitter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2de395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jitter(values, jitter=0.5):\n",
    "    n = len(values)\n",
    "    return np.random.uniform(-jitter, +jitter, n) + values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ef7d8f",
   "metadata": {},
   "source": [
    "The values can be any sequence; the result is a NumPy array.\n",
    "\n",
    "Figure [\\[scatter1\\]](#scatter1){reference-type=\"ref\"\n",
    "reference=\"scatter1\"} (right) shows the result. Jittering reduces the\n",
    "visual effect of rounding and makes the shape of the relationship\n",
    "clearer. But in general you should only jitter data for purposes of\n",
    "visualization and avoid using jittered data for analysis.\n",
    "\n",
    "Even with jittering, this is not the best way to represent the data.\n",
    "There are many overlapping points, which hides data in the dense parts\n",
    "of the figure and gives disproportionate emphasis to outliers. This\n",
    "effect is called **saturation**.\n",
    "\n",
    "![Scatter plot with jittering and transparency (left), hexbin plot\n",
    "(right).](figs/scatter2.pdf){height=\"3.0in\"}\n",
    "\n",
    "We can solve this problem with the `alpha` parameter, which makes the\n",
    "points partly transparent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8052cf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "thinkplot.scatter(heights, weights, alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4830f7e8",
   "metadata": {},
   "source": [
    "Figure [\\[scatter2\\]](#scatter2){reference-type=\"ref\"\n",
    "reference=\"scatter2\"} (left) shows the result. Overlapping data points\n",
    "look darker, so darkness is proportional to density. In this version of\n",
    "the plot we can see two details that were not apparent before: vertical\n",
    "clusters at several heights and a horizontal line near 90 kg or 200\n",
    "pounds. Since this data is based on self-reports in pounds, the most\n",
    "likely explanation is that some respondents reported rounded values.\n",
    "\n",
    "Using transparency works well for moderate-sized datasets, but this\n",
    "figure only shows the first 5000 records in the BRFSS, out of a total of\n",
    "414 509.\n",
    "\n",
    "To handle larger datasets, another option is a hexbin plot, which\n",
    "divides the graph into hexagonal bins and colors each bin according to\n",
    "how many data points fall in it. `thinkplot` provides `HexBin`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8961004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "thinkplot.hex_bin(heights, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a46b3f7",
   "metadata": {},
   "source": [
    "Figure [\\[scatter2\\]](#scatter2){reference-type=\"ref\"\n",
    "reference=\"scatter2\"} (right) shows the result. An advantage of a hexbin\n",
    "is that it shows the shape of the relationship well, and it is efficient\n",
    "for large datasets, both in time and in the size of the file it\n",
    "generates. A drawback is that it makes the outliers invisible.\n",
    "\n",
    "The point of this example is that it is not easy to make a scatter plot\n",
    "that shows relationships clearly without introducing misleading\n",
    "artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e28ec5",
   "metadata": {},
   "source": [
    "## Characterizing relationships\n",
    "\n",
    "Scatter plots provide a general impression of the relationship between\n",
    "variables, but there are other visualizations that provide more insight\n",
    "into the nature of the relationship. One option is to bin one variable\n",
    "and plot percentiles of the other.\n",
    "\n",
    "NumPy and pandas provide functions for binning data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90d72287",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"htm3\", \"wtkg2\"])\n",
    "bins = np.arange(135, 210, 5)\n",
    "indices = np.digitize(df.htm3, bins)\n",
    "groups = df.groupby(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a67650",
   "metadata": {},
   "source": [
    "`dropna` drops rows with `nan` in any of the listed columns. `arange`\n",
    "makes a NumPy array of bins from 135 to, but not including, 210, in\n",
    "increments of 5.\n",
    "\n",
    "`digitize` computes the index of the bin that contains each value in\n",
    "`df.htm3`. The result is a NumPy array of integer indices. Values that\n",
    "fall below the lowest bin are mapped to index 0. Values above the\n",
    "highest bin are mapped to `len(bins)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc68dda",
   "metadata": {},
   "source": [
    "`groupby` is a DataFrame method that returns a GroupBy object; used in a\n",
    "`for` loop, `groups` iterates the names of the groups and the DataFrames\n",
    "that represent them. So, for example, we can print the number of rows in\n",
    "each group like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3fd9732",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, group in groups:\n",
    "    print(i, len(group))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8f1ebd",
   "metadata": {},
   "source": [
    "Now for each group we can compute the mean height and the CDF of weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9c3fa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "heights = [group.htm3.mean() for i, group in groups]\n",
    "cdfs = [thinkstats2.Cdf(group.wtkg2) for i, group in groups]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5344bb25",
   "metadata": {},
   "source": [
    "Finally, we can plot percentiles of weight versus height:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e5d5263",
   "metadata": {},
   "outputs": [],
   "source": [
    "for percent in [75, 50, 25]:\n",
    "    weights = [cdf.percentile(percent) for cdf in cdfs]\n",
    "    label = \"%dth\" % percent\n",
    "    thinkplot.plot(heights, weights, label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da71b22",
   "metadata": {},
   "source": [
    "Figure [\\[scatter3\\]](#scatter3){reference-type=\"ref\"\n",
    "reference=\"scatter3\"} shows the result. Between 140 and 200 cm the\n",
    "relationship between these variables is roughly linear. This range\n",
    "includes more than 99% of the data, so we don't have to worry too much\n",
    "about the extremes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503b7c6e",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "\n",
    "A **correlation** is a statistic intended to quantify the strength of\n",
    "the relationship between two variables.\n",
    "\n",
    "A challenge in measuring correlation is that the variables we want to\n",
    "compare are often not expressed in the same units. And even if they are\n",
    "in the same units, they come from different distributions.\n",
    "\n",
    "There are two common solutions to these problems:\n",
    "\n",
    "1.  Transform each value to a **standard score**, which is the number of\n",
    "    standard deviations from the mean. This transform leads to the\n",
    "    \"Pearson product-moment correlation coefficient.\"\n",
    "\n",
    "2.  Transform each value to its **rank**, which is its index in the\n",
    "    sorted list of values. This transform leads to the \"Spearman rank\n",
    "    correlation coefficient.\"\n",
    "\n",
    "If $X$ is a series of $n$ values, $x_i$, we can convert to standard\n",
    "scores by subtracting the mean and dividing by the standard deviation:\n",
    "$z_i = (x_i - \\mu) / \\sigma$.\n",
    "\n",
    "The numerator is a deviation: the distance from the mean. Dividing by\n",
    "$\\sigma$ **standardizes** the deviation, so the values of $Z$ are\n",
    "dimensionless (no units) and their distribution has mean 0 and variance\n",
    "1.\n",
    "\n",
    "If $X$ is normally distributed, so is $Z$. But if $X$ is skewed or has\n",
    "outliers, so does $Z$; in those cases, it is more robust to use\n",
    "percentile ranks. If we compute a new variable, $R$, so that $r_i$ is\n",
    "the rank of $x_i$, the distribution of $R$ is uniform from 1 to $n$,\n",
    "regardless of the distribution of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0ef47d",
   "metadata": {},
   "source": [
    "## Covariance\n",
    "\n",
    "**Covariance** is a measure of the tendency of two variables to vary\n",
    "together. If we have two series, $X$ and $Y$, their deviations from the\n",
    "mean are $$dx_i = x_i - \\bar{x}$$ $$dy_i = y_i - \\ybar$$ where $\\bar{x}$ is\n",
    "the sample mean of $X$ and $\\ybar$ is the sample mean of $Y$. If $X$ and\n",
    "$Y$ vary together, their deviations tend to have the same sign.\n",
    "\n",
    "If we multiply them together, the product is positive when the\n",
    "deviations have the same sign and negative when they have the opposite\n",
    "sign. So adding up the products gives a measure of the tendency to vary\n",
    "together.\n",
    "\n",
    "Covariance is the mean of these products:\n",
    "$$Cov(X,Y) = \\frac{1}{n} \\sum dx_i~dy_i$$ where $n$ is the length of the\n",
    "two series (they have to be the same length).\n",
    "\n",
    "If you have studied linear algebra, you might recognize that `Cov` is\n",
    "the dot product of the deviations, divided by their length. So the\n",
    "covariance is maximized if the two vectors are identical, 0 if they are\n",
    "orthogonal, and negative if they point in opposite directions.\n",
    "`thinkstats2` uses `np.dot` to implement `Cov` efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bc8627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov(xs, ys, meanx=None, meany=None):\n",
    "    xs = np.asarray(xs)\n",
    "    ys = np.asarray(ys)\n",
    "    if meanx is None:\n",
    "        meanx = np.mean(xs)\n",
    "    if meany is None:\n",
    "        meany = np.mean(ys)\n",
    "    cov = np.dot(xs - meanx, ys - meany) / len(xs)\n",
    "    return cov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce97c1ff",
   "metadata": {},
   "source": [
    "By default `Cov` computes deviations from the sample means, or you can\n",
    "provide known means. If `xs` and `ys` are Python sequences, `np.asarray`\n",
    "converts them to NumPy arrays. If they are already NumPy arrays,\n",
    "`np.asarray` does nothing.\n",
    "\n",
    "This implementation of covariance is meant to be simple for purposes of\n",
    "explanation. NumPy and pandas also provide implementations of\n",
    "covariance, but both of them apply a correction for small sample sizes\n",
    "that we have not covered yet, and `np.cov` returns a covariance matrix,\n",
    "which is more than we need for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5890f86b",
   "metadata": {},
   "source": [
    "## Pearson's correlation\n",
    "\n",
    "Covariance is useful in some computations, but it is seldom reported as\n",
    "a summary statistic because it is hard to interpret. Among other\n",
    "problems, its units are the product of the units of $X$ and $Y$. For\n",
    "example, the covariance of weight and height in the BRFSS dataset is 113\n",
    "kilogram-centimeters, whatever that means.\n",
    "\n",
    "One solution to this problem is to divide the deviations by the standard\n",
    "deviation, which yields standard scores, and compute the product of\n",
    "standard scores:\n",
    "$$p_i = \\frac{(x_i - \\bar{x})}{S_X} \\frac{(y_i - \\ybar)}{S_Y}$$ Where\n",
    "$S_X$ and $S_Y$ are the standard deviations of $X$ and $Y$. The mean of\n",
    "these products is $$\\rho = \\frac{1}{n} \\sum p_i$$ Or we can rewrite\n",
    "$\\rho$ by factoring out $S_X$ and $S_Y$:\n",
    "$$\\rho = \\frac{Cov(X,Y)}{S_X S_Y}$$ This value is called **Pearson's\n",
    "correlation** after Karl Pearson, an influential early statistician. It\n",
    "is easy to compute and easy to interpret. Because standard scores are\n",
    "dimensionless, so is $\\rho$.\n",
    "\n",
    "Here is the implementation in `thinkstats2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d70ecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thinkstats2 import mean_var\n",
    "\n",
    "\n",
    "def corr(xs, ys):\n",
    "    xs = np.asarray(xs)\n",
    "    ys = np.asarray(ys)\n",
    "    meanx, varx = mean_var(xs)\n",
    "    meany, vary = mean_var(ys)\n",
    "    corr = cov(xs, ys, meanx, meany) / np.sqrt(varx * vary)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a688d5ae",
   "metadata": {},
   "source": [
    "`MeanVar` computes mean and variance slightly more efficiently than\n",
    "separate calls to `np.mean` and `np.var`.\n",
    "\n",
    "Pearson's correlation is always between -1 and +1 (including both). If\n",
    "$\\rho$ is positive, we say that the correlation is positive, which means\n",
    "that when one variable is high, the other tends to be high. If $\\rho$ is\n",
    "negative, the correlation is negative, so when one variable is high, the\n",
    "other is low.\n",
    "\n",
    "The magnitude of $\\rho$ indicates the strength of the correlation. If\n",
    "$\\rho$ is 1 or -1, the variables are perfectly correlated, which means\n",
    "that if you know one, you can make a perfect prediction about the other.\n",
    "\n",
    "Most correlation in the real world is not perfect, but it is still\n",
    "useful. The correlation of height and weight is 0.51, which is a strong\n",
    "correlation compared to similar human-related variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b55864",
   "metadata": {},
   "source": [
    "## Nonlinear relationships\n",
    "\n",
    "If Pearson's correlation is near 0, it is tempting to conclude that\n",
    "there is no relationship between the variables, but that conclusion is\n",
    "not valid. Pearson's correlation only measures *linear* relationships.\n",
    "If there's a nonlinear relationship, $\\rho$ understates its strength."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c249bb",
   "metadata": {},
   "source": [
    "![Examples of datasets with a range of\n",
    "correlations.](figs/Correlation_examples.png){height=\"2.5in\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6706b846",
   "metadata": {},
   "source": [
    "Figure [\\[corr_examples\\]](#corr_examples){reference-type=\"ref\"\n",
    "reference=\"corr_examples\"} is from\n",
    "<http://wikipedia.org/wiki/Correlation_and_dependence>. It shows scatter\n",
    "plots and correlation coefficients for several carefully constructed\n",
    "datasets.\n",
    "\n",
    "The top row shows linear relationships with a range of correlations; you\n",
    "can use this row to get a sense of what different values of $\\rho$ look\n",
    "like. The second row shows perfect correlations with a range of slopes,\n",
    "which demonstrates that correlation is unrelated to slope (we'll talk\n",
    "about estimating slope soon). The third row shows variables that are\n",
    "clearly related, but because the relationship is nonlinear, the\n",
    "correlation coefficient is 0.\n",
    "\n",
    "The moral of this story is that you should always look at a scatter plot\n",
    "of your data before blindly computing a correlation coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b20d965",
   "metadata": {},
   "source": [
    "## Spearman's rank correlation\n",
    "\n",
    "Pearson's correlation works well if the relationship between variables\n",
    "is linear and if the variables are roughly normal. But it is not robust\n",
    "in the presence of outliers. Spearman's rank correlation is an\n",
    "alternative that mitigates the effect of outliers and skewed\n",
    "distributions. To compute Spearman's correlation, we have to compute the\n",
    "**rank** of each value, which is its index in the sorted sample. For\n",
    "example, in the sample `[1, 2, 5, 7]` the rank of the value 5 is 3,\n",
    "because it appears third in the sorted list. Then we compute Pearson's\n",
    "correlation for the ranks.\n",
    "\n",
    "`thinkstats2` provides a function that computes Spearman's rank\n",
    "correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76d478e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def spearman_corr(xs, ys):\n",
    "    xranks = pd.Series(xs).rank()\n",
    "    yranks = pd.Series(ys).rank()\n",
    "    return corr(xranks, yranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5056090",
   "metadata": {},
   "source": [
    "I convert the arguments to pandas Series objects so I can use `rank`,\n",
    "which computes the rank for each value and returns a Series. Then I use\n",
    "`Corr` to compute the correlation of the ranks.\n",
    "\n",
    "I could also use `Series.corr` directly and specify Spearman's method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3ef1610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman_corr(xs, ys):\n",
    "    xs = pd.Series(xs)\n",
    "    ys = pd.Series(ys)\n",
    "    return xs.corr(ys, method=\"spearman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde7a6ca",
   "metadata": {},
   "source": [
    "The Spearman rank correlation for the BRFSS data is 0.54, a little\n",
    "higher than the Pearson correlation, 0.51. There are several possible\n",
    "reasons for the difference, including:\n",
    "\n",
    "-   If the relationship is nonlinear, Pearson's correlation tends to\n",
    "    underestimate the strength of the relationship, and\n",
    "\n",
    "-   Pearson's correlation can be affected (in either direction) if one\n",
    "    of the distributions is skewed or contains outliers. Spearman's rank\n",
    "    correlation is more robust.\n",
    "\n",
    "In the BRFSS example, we know that the distribution of weights is\n",
    "roughly lognormal; under a log transform it approximates a normal\n",
    "distribution, so it has no skew. So another way to eliminate the effect\n",
    "of skewness is to compute Pearson's correlation with log-weight and\n",
    "height:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3823059",
   "metadata": {},
   "outputs": [],
   "source": [
    "thinkstats2.corr(df.htm3, np.log(df.wtkg2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27463511",
   "metadata": {},
   "source": [
    "The result is 0.53, close to the rank correlation, 0.54. So that\n",
    "suggests that skewness in the distribution of weight explains most of\n",
    "the difference between Pearson's and Spearman's correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4ace1e",
   "metadata": {},
   "source": [
    "## Correlation and causation\n",
    "\n",
    "If variables A and B are correlated, there are three possible\n",
    "explanations: A causes B, or B causes A, or some other set of factors\n",
    "causes both A and B. These explanations are called \"causal\n",
    "relationships\".\n",
    "\n",
    "Correlation alone does not distinguish between these explanations, so it\n",
    "does not tell you which ones are true. This rule is often summarized\n",
    "with the phrase \"Correlation does not imply causation,\" which is so\n",
    "pithy it has its own Wikipedia page:\n",
    "<http://wikipedia.org/wiki/Correlation_does_not_imply_causation>.\n",
    "\n",
    "So what can you do to provide evidence of causation?\n",
    "\n",
    "1.  Use time. If A comes before B, then A can cause B but not the other\n",
    "    way around (at least according to our common understanding of\n",
    "    causation). The order of events can help us infer the direction of\n",
    "    causation, but it does not preclude the possibility that something\n",
    "    else causes both A and B.\n",
    "\n",
    "2.  Use randomness. If you divide a large sample into two groups at\n",
    "    random and compute the means of almost any variable, you expect the\n",
    "    difference to be small. If the groups are nearly identical in all\n",
    "    variables but one, you can eliminate spurious relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ec068",
   "metadata": {},
   "source": [
    "This works even if you don't know what the relevant variables are,\n",
    "but it works even better if you do, because you can check that the\n",
    "groups are identical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e5d580",
   "metadata": {},
   "source": [
    "These ideas are the motivation for the **randomized controlled trial**,\n",
    "in which subjects are assigned randomly to two (or more) groups: a\n",
    "**treatment group** that receives some kind of intervention, like a new\n",
    "medicine, and a **control group** that receives no intervention, or\n",
    "another treatment whose effects are known.\n",
    "\n",
    "A randomized controlled trial is the most reliable way to demonstrate a\n",
    "causal relationship, and the foundation of science-based medicine (see\n",
    "<http://wikipedia.org/wiki/Randomized_controlled_trial>).\n",
    "\n",
    "Unfortunately, controlled trials are only possible in the laboratory\n",
    "sciences, medicine, and a few other disciplines. In the social sciences,\n",
    "controlled experiments are rare, usually because they are impossible or\n",
    "unethical.\n",
    "\n",
    "An alternative is to look for a **natural experiment**, where different\n",
    "\"treatments\" are applied to groups that are otherwise similar. One\n",
    "danger of natural experiments is that the groups might differ in ways\n",
    "that are not apparent. You can read more about this topic at\n",
    "<http://wikipedia.org/wiki/Natural_experiment>.\n",
    "\n",
    "In some cases it is possible to infer causal relationships using\n",
    "**regression analysis**, which is the topic of\n",
    "Chapter [\\[regression\\]](#regression){reference-type=\"ref\"\n",
    "reference=\"regression\"}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4586192",
   "metadata": {},
   "source": [
    "## Glossary\n",
    "\n",
    "-   **scatter plot**: A visualization of the relationship between two\n",
    "    variables, showing one point for each row of data.\n",
    "\n",
    "-   **jitter**: Random noise added to data for purposes of\n",
    "    visualization.\n",
    "\n",
    "-   **saturation**: Loss of information when multiple points are plotted\n",
    "    on top of each other.\n",
    "\n",
    "-   **correlation**: A statistic that measures the strength of the\n",
    "    relationship between two variables.\n",
    "\n",
    "-   **standardize**: To transform a set of values so that their mean is\n",
    "    0 and their variance is 1.\n",
    "\n",
    "-   **standard score**: A value that has been standardized so that it is\n",
    "    expressed in standard deviations from the mean.\n",
    "\n",
    "-   **covariance**: A measure of the tendency of two variables to vary\n",
    "    together.\n",
    "\n",
    "-   **rank**: The index where an element appears in a sorted list.\n",
    "\n",
    "-   **randomized controlled trial**: An experimental design in which\n",
    "    subjects are divided into groups at random, and different groups are\n",
    "    given different treatments.\n",
    "\n",
    "-   **treatment group**: A group in a controlled trial that receives\n",
    "    some kind of intervention.\n",
    "\n",
    "-   **control group**: A group in a controlled trial that receives no\n",
    "    treatment, or a treatment whose effect is known.\n",
    "\n",
    "-   **natural experiment**: An experimental design that takes advantage\n",
    "    of a natural division of subjects into groups in ways that are at\n",
    "    least approximately random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25da22f6",
   "metadata": {},
   "source": [
    "**Exercise:**  So far we have been working with a subset of only 5000 respondents.  When we include the entire dataset, making an effective scatter plot can be tricky.  As an exercise, experiment with `Scatter` and `HexBin` to make a plot that represents the entire dataset well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8918c1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "heights = jitter(df.htm3, 2.8)\n",
    "weights = jitter(df.wtkg2, 1.0)\n",
    "thinkplot.scatter(heights, weights, alpha=0.01, s=2)\n",
    "thinkplot.config(\n",
    "    xlabel=\"Height (cm)\", ylabel=\"Weight (kg)\", axis=[140, 210, 20, 200], legend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52dad78",
   "metadata": {},
   "source": [
    "**Exercise:** Yet another option is to divide the dataset into groups and then plot the CDF for each group.  As an exercise, divide the dataset into a smaller number of groups and plot the CDF for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8573793",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = df.dropna(subset=[\"htm3\", \"wtkg2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66c5d783",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(140, 210, 10)\n",
    "indices = np.digitize(cleaned.htm3, bins)\n",
    "groups = cleaned.groupby(indices)\n",
    "cdfs = [thinkstats2.Cdf(group.wtkg2) for i, group in groups]\n",
    "thinkplot.pre_plot(len(cdfs))\n",
    "thinkplot.cdfs(cdfs)\n",
    "thinkplot.config(xlabel=\"Weight (kg)\", ylabel=\"CDF\", axis=[20, 200, 0, 1], legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eff46b",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cfdd57",
   "metadata": {},
   "source": [
    "Using data from the NSFG, make a scatter plot of birth weight versus mother’s age. Plot percentiles of birth weight versus mother’s age. Compute Pearson’s and Spearman’s correlations. How would you characterize the relationship between these variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65016a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "download(\"https://github.com/AllenDowney/ThinkStats/raw/v3/nb/nsfg.py\")\n",
    "download(\"https://github.com/AllenDowney/ThinkStats/raw/v3/nb/2002FemPreg.dct\")\n",
    "download(\n",
    "    \"https://github.com/AllenDowney/ThinkStats/raw/v3/nb/2002FemPreg.dat.gz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47c256b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nsfg\n",
    "\n",
    "live, firsts, others = nsfg.make_frames()\n",
    "live = live.dropna(subset=[\"agepreg\", \"totalwgt_lb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "662c6094",
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = live.agepreg\n",
    "weights = live.totalwgt_lb\n",
    "print(\"Corr\", corr(ages, weights))\n",
    "print(\"SpearmanCorr\", spearman_corr(ages, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc5960fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binned_percentiles(df):\n",
    "    \"\"\"Bin the data by age and plot percentiles of weight for each bin.\n",
    "\n",
    "    df: DataFrame\n",
    "    \"\"\"\n",
    "    bins = np.arange(10, 48, 3)\n",
    "    indices = np.digitize(df.agepreg, bins)\n",
    "    groups = df.groupby(indices)\n",
    "    ages = [group.agepreg.mean() for i, group in groups][1:-1]\n",
    "    cdfs = [thinkstats2.Cdf(group.totalwgt_lb) for i, group in groups][1:-1]\n",
    "    thinkplot.pre_plot(3)\n",
    "    for percent in [75, 50, 25]:\n",
    "        weights = [cdf.percentile(percent) for cdf in cdfs]\n",
    "        label = \"%dth\" % percent\n",
    "        thinkplot.plot(ages, weights, label=label)\n",
    "    thinkplot.config(\n",
    "        xlabel=\"Mother's age (years)\",\n",
    "        ylabel=\"Birth weight (lbs)\",\n",
    "        xlim=[14, 45],\n",
    "        legend=True,\n",
    "    )\n",
    "\n",
    "\n",
    "binned_percentiles(live)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c86e915",
   "metadata": {},
   "outputs": [],
   "source": [
    "thinkplot.scatter(ages, weights, alpha=0.05, s=10)\n",
    "thinkplot.config(\n",
    "    xlabel=\"Age (years)\",\n",
    "    ylabel=\"Birth weight (lbs)\",\n",
    "    xlim=[10, 45],\n",
    "    ylim=[0, 15],\n",
    "    legend=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890e2c89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
